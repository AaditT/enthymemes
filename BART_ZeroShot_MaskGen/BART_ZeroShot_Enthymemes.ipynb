{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BART_ZeroShot_Enthymemes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig5xzKFpLcHl",
        "outputId": "b7ecb03b-ac2a-433a-cfb2-db1185cb2cdd"
      },
      "source": [
        "!pip install hydra-core"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core) (5.1.2)\n",
            "Collecting omegaconf<2.1,>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 25.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core) (3.4.1)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1,>=2.0.5->hydra-core) (3.7.4.3)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=e6f0d035e600f71853065ffdbf4e80bed48811b2030e60938169518a7e44fe18\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, omegaconf, antlr4-python3-runtime, hydra-core\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 hydra-core-1.0.6 omegaconf-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgwV-3_hK1qo"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTeeynPMLMkY",
        "outputId": "250b4fac-b078-4007-ce18-57f5e57f98f9"
      },
      "source": [
        "bart = torch.hub.load('pytorch/fairseq', 'bart.large')\n",
        "bart.eval()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/fairseq/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "cythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:369: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\n",
            "building 'fairseq.libbleu' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/fairseq\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/clib\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/clib/libbleu\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/fairseq\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.data_utils_fast' extension\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/data\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "creating build/lib.linux-x86_64-3.7/fairseq/data\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o -o build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.token_block_utils_fast' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o -o build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'fairseq.libnat' extension\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/clib/libnat\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so -> fairseq\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3699866548/3699866548 [01:51<00:00, 33288783.72B/s]\n",
            "1042301B [00:00, 3576745.49B/s]\n",
            "456318B [00:00, 10315102.98B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BARTHubInterface(\n",
              "  (models): ModuleList(\n",
              "    (0): BARTModel(\n",
              "      (encoder): TransformerEncoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (6): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (7): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (8): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (9): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (10): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (11): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (decoder): TransformerDecoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (6): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (7): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (8): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (9): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (10): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (11): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
              "      )\n",
              "      (classification_heads): ModuleDict()\n",
              "    )\n",
              "  )\n",
              "  (model): BARTModel(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (dropout_module): FairseqDropout()\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (dropout_module): FairseqDropout()\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
              "    )\n",
              "    (classification_heads): ModuleDict()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx_7HC8FLyOq"
      },
      "source": [
        "x = bart.fill_mask([\"Gina's friend Tami had a folder that Gina wanted. And since <mask> <mask> <mask>. So she decided she would try to find the folder on her own.\"], topk=1, beam=10, match_source_len=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8DU6N2tPDya",
        "outputId": "2c5036a8-c3ad-4d4e-9ee9-ad6b3fc60681"
      },
      "source": [
        "x[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"Gina's friend Tami had a folder that Gina wanted. And since Tami was gone, Gina didn't know where it was. So she decided she would try to find the folder on her own.\",\n",
              "  tensor(-0.5008))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_b_TWBVwQJ5f",
        "outputId": "819bbd1d-ba1c-4df2-e8ba-ab1a5c8cc2ca"
      },
      "source": [
        "x[0][1][0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Gina's friend Tami had a folder that Gina wanted. And since Tami wasn't around, Gina didn't know where it was. So she decided she would try to find the folder on her own.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scry-iLoM4LD"
      },
      "source": [
        "val_lines_list = []\n",
        "val_file = open('val.source')\n",
        "val_lines = val_file.readlines()\n",
        "for line in val_lines:\n",
        "  val_lines_list.append(line)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPj8lzARNgro"
      },
      "source": [
        "val_lines_list2 = []\n",
        "for line in val_lines_list:\n",
        "  insert_str = \"And since <mask> <mask> <mask>.\"\n",
        "  line_blocks = line.split(\"#\")\n",
        "  if (len(line_blocks) == 2): # if there is only one #\n",
        "    new_str = line_blocks[0] + insert_str + line_blocks[1]\n",
        "    val_lines_list2.append(new_str)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKCfMYI2Qdot",
        "outputId": "b8b7bf44-0164-47c5-cad7-dac05ef4c949"
      },
      "source": [
        "len(val_lines_list2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14313"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2RccekoQqgj",
        "outputId": "d6e162aa-2292-4b15-9f4d-cce3daf9857b"
      },
      "source": [
        "print(56/14313)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.003912527073290016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir5j6rI9PvWt",
        "outputId": "4cf03f89-015e-443c-8861-3eea6181ea5d"
      },
      "source": [
        "target_lines = []\n",
        "counter = 0\n",
        "limit = 1000\n",
        "temp_line = \"\"\n",
        "for masked_line in val_lines_list2:\n",
        "  if (counter < limit):\n",
        "    if (temp_line == masked_line):\n",
        "      counter += 1\n",
        "      continue\n",
        "    x = bart.fill_mask([masked_line], topk=1, beam=10, match_source_len=False)\n",
        "    target_lines.append(x[0][0][0])\n",
        "    print(x[0][0][0])\n",
        "    counter += 1\n",
        "    print(str(counter) + \" / \" + str(limit) + \"   ---   \" + str(round(counter/limit, 4)) + \"%\")\n",
        "    print()\n",
        "    print()\n",
        "    temp_line = masked_line\n",
        "  else:\n",
        "    break"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Smiths were having photos done of the children. Ty's face lit up as he ran to the new toy, happily posing for photos. And since then, he's been playing with it ever since.\n",
            "1 / 1000   ---   0.001%\n",
            "\n",
            "\n",
            "Timmy was running fast to class. Timmy later found out that the exam was to be on time, he failed. And since then, Timmy has never been the same.\n",
            "6 / 1000   ---   0.006%\n",
            "\n",
            "\n",
            "I was in line at the cinema. The boy proved her wrong by wetting himself. And since then, I’ve never been the same.\n",
            "10 / 1000   ---   0.01%\n",
            "\n",
            "\n",
            "Zoey is an only child. And since she's an only child, she can't wait to meet her brother. When the baby is born, she goes to the hospital to meet her brother!\n",
            "13 / 1000   ---   0.013%\n",
            "\n",
            "\n",
            "Hannah sent her kids to go to the shoreline. As a result, they had to get out of the water and put on sunscreen. And since it was so hot, they didn't want to get wet.\n",
            "17 / 1000   ---   0.017%\n",
            "\n",
            "\n",
            "It was a cold night and the campers were tired and hungry. They went off to their tents after a few hours to sleep. And since then, they have never looked back.\n",
            "22 / 1000   ---   0.022%\n",
            "\n",
            "\n",
            "Amy was in the park walking her dog Spot. Amy gave the mime a $5 to make up for being scared by Spot. And since then, the mime has been a regular at the park.\n",
            "26 / 1000   ---   0.026%\n",
            "\n",
            "\n",
            "Jim was not confident in his home repair skills. Jim was so excited to learn a new skill. And since then, he has never looked back.\n",
            "29 / 1000   ---   0.029%\n",
            "\n",
            "\n",
            "We used to go to the park all the time. He needed 19 Stitches that night. And since then, he's never been the same.\n",
            "33 / 1000   ---   0.033%\n",
            "\n",
            "\n",
            "Music was Kat's life. She completed her studies with honors, and is now a renowned pianist. And since then, she's been teaching others how to play.\n",
            "37 / 1000   ---   0.037%\n",
            "\n",
            "\n",
            "Paul and Jessica had been married for 15 Year's. Paul and Jessica finally went their separate ways. And since then, they haven't spoken to each other.\n",
            "41 / 1000   ---   0.041%\n",
            "\n",
            "\n",
            "Joe was an aspiring rap artist. Joe eventually became a local celebrity for his music. And since then, he’s never looked back.\n",
            "46 / 1000   ---   0.046%\n",
            "\n",
            "\n",
            "Paul didn't want to go to school today. Now Paul is grounded for two weeks. And since he didn't want to go to school...\n",
            "50 / 1000   ---   0.05%\n",
            "\n",
            "\n",
            "Riley went to the store with her mother. Riley wore her cowboy boots to school the next day. And since then, she’s been wearing them every day.\n",
            "55 / 1000   ---   0.055%\n",
            "\n",
            "\n",
            "Cindy was given a glass angel for Christmas. And since then, the angel has disappeared. Cindy has never been so devastated.\n",
            "59 / 1000   ---   0.059%\n",
            "\n",
            "\n",
            "Brian had a crush on Teresa. He spent the rest of the day moping. And since then, he hasn’t been the same.\n",
            "63 / 1000   ---   0.063%\n",
            "\n",
            "\n",
            "Ray hung a tire on a rope to make his daughter a swing. Ray laughed at the crazy situation. And since then, he’s been doing it every day.\n",
            "67 / 1000   ---   0.067%\n",
            "\n",
            "\n",
            "Ray hung a tire on a rope to make his daughter a swing. Ray ran to his daughter to make sure she was okay. And since then, he has never looked back.\n",
            "69 / 1000   ---   0.069%\n",
            "\n",
            "\n",
            "The hubby and I decided to have a date night tonight. We thanked Mama for her huge favor. And since then, we haven't looked back.\n",
            "72 / 1000   ---   0.072%\n",
            "\n",
            "\n",
            "Jordan and his girlfriend of three Year's decided to move in together. In the end, they decided to break up and move on. And since then, Jordan and his girlfriend have been inseparable.\n",
            "75 / 1000   ---   0.075%\n",
            "\n",
            "\n",
            "Becs sat down to start her novel on Sunday morning. For the next 6 hours, she wrote without stopping. And since then, she hasn't stopped.\n",
            "79 / 1000   ---   0.079%\n",
            "\n",
            "\n",
            "Mike spends a lot of his time on the internet. Now other people love the internet because of Mike's website. And since Mike loves the internet, so do we.\n",
            "84 / 1000   ---   0.084%\n",
            "\n",
            "\n",
            "I am visiting my mother-in-law this weekend. I ordered a big alarm clock that has good reviews. And since I am staying with my mother-in-law, I...\n",
            "87 / 1000   ---   0.087%\n",
            "\n",
            "\n",
            "I was talking to my co-worker about a problem in our workplace. She ended up calling my boss and I was written up the next day. And since then, I haven't been able to get a job.\n",
            "92 / 1000   ---   0.092%\n",
            "\n",
            "\n",
            "Susan loves to ride her horse on the nearby trails. Susan needed stitches to close the cut on her head. And since then, she’s been able to ride her horse again.\n",
            "96 / 1000   ---   0.096%\n",
            "\n",
            "\n",
            "Allan hated gym class. And since then, he's never been the same. Allan never hated gym class again!\n",
            "101 / 1000   ---   0.101%\n",
            "\n",
            "\n",
            "Tom loved to eat cheerios. Tom decided that cheerios are not good with water. And since then, cheerios are not good with water.\n",
            "106 / 1000   ---   0.106%\n",
            "\n",
            "\n",
            "I ordered peonies for my girlfriend for Valentines day. They sent a new batch out the next day that were so great. And since then I’ve been getting them every year.\n",
            "110 / 1000   ---   0.11%\n",
            "\n",
            "\n",
            "Cara had a crush on a boy in her English class named Ray. And since they were in the same class, they decided to do a project together. Soon they began dating - and they got a great grade on their project!\n",
            "114 / 1000   ---   0.114%\n",
            "\n",
            "\n",
            "John is really good at playing chess. Eventually Mary won, Mark was right, and John was shocked. And since then, he’s never played chess again.\n",
            "118 / 1000   ---   0.118%\n",
            "\n",
            "\n",
            "The children were inside playing when they heard music. The children excitedly bought ice cream cones. And since then, they have been enjoying their ice cream.\n",
            "122 / 1000   ---   0.122%\n",
            "\n",
            "\n",
            "I was trying to tell my uncle a message. He didn't say a single word to me for the rest of the day. And since then, I haven't heard a word from him.\n",
            "125 / 1000   ---   0.125%\n",
            "\n",
            "\n",
            "I bought a great pair of red shoe at the shoe store. I ended up getting a white pair with no heels. And since then, I’ve been wearing them all the time.\n",
            "129 / 1000   ---   0.129%\n",
            "\n",
            "\n",
            "After Mike was legally allowed to drink, he formed some bad habits. Mike decided to stop drinking alcohol from then on. And since then, he hasn’t had a drink since.\n",
            "133 / 1000   ---   0.133%\n",
            "\n",
            "\n",
            "My aunt took a family portrait of everyone at the party. I didn't bother to ask for a copy and ended up walking away. And since then, I haven't been able to find it.\n",
            "137 / 1000   ---   0.137%\n",
            "\n",
            "\n",
            "Micah was at work. He was never able to find it. And since then, nothing has been the same.\n",
            "140 / 1000   ---   0.14%\n",
            "\n",
            "\n",
            "Raya went to visit her family in Iran. She was glad to return home to America. And since then, she hasn’t looked back.\n",
            "145 / 1000   ---   0.145%\n",
            "\n",
            "\n",
            "Amber and Rob had taken some time apart And since then, they’ve been spending more time together. Rob and Amber decided to move back in together\n",
            "149 / 1000   ---   0.149%\n",
            "\n",
            "\n",
            "Gina's friend Tami had a folder that Gina wanted. And since Tami was gone, Gina didn't know where it was. So she decided she would try to find the folder on her own.\n",
            "153 / 1000   ---   0.153%\n",
            "\n",
            "\n",
            "It was my birthday. When I got home the party was set up for my brother. And since it was my birthday, I was invited.\n",
            "157 / 1000   ---   0.157%\n",
            "\n",
            "\n",
            "Ritz was born in a large litter of 12 other puppies. Ritz was orphaned and had to be sent to a shelter. And since then, he’s become one of the city’s favorite dogs.\n",
            "161 / 1000   ---   0.161%\n",
            "\n",
            "\n",
            "Joe lived a lonely life. Joe ordered some new cable channels, but was still unhappy. And since then, Joe’s life has never been the same.\n",
            "165 / 1000   ---   0.165%\n",
            "\n",
            "\n",
            "Ginny was playing ball in the house. Ginny's mom grounded her. And since then, Ginny has been grounded.\n",
            "168 / 1000   ---   0.168%\n",
            "\n",
            "\n",
            "I wanted some cabinets to put in my bedroom. And since I didn’t have any, I decided to make them myself. After two months of work I had crafted some beautiful cabinets!\n",
            "173 / 1000   ---   0.173%\n",
            "\n",
            "\n",
            "In the autumn, Susan planted some flower seeds in the backyard. She then dug up all her flowers. And since then, she hasn't been able to find any.\n",
            "177 / 1000   ---   0.177%\n",
            "\n",
            "\n",
            "Francis has always looked forward to learning how to drive. Francis decides he would rather take public transportation. And since then, he hasn’t looked back.\n",
            "179 / 1000   ---   0.179%\n",
            "\n",
            "\n",
            "Amanda needed to get across town to the store. And since she didn’t have a car, she borrowed a friend’s. She got in and they were off to the store!\n",
            "182 / 1000   ---   0.182%\n",
            "\n",
            "\n",
            "Mary makes candles. Mary felt unappreciated. And since then, nothing has changed.\n",
            "185 / 1000   ---   0.185%\n",
            "\n",
            "\n",
            "Lara was having a birthday party. Tina was upset she was late. And since she was late, she was late.\n",
            "188 / 1000   ---   0.188%\n",
            "\n",
            "\n",
            "Cory went down a very steep hill on a longboard. He was picked up by a car and taken to the hospital. And since then, he’s been in and out of the hospital.\n",
            "192 / 1000   ---   0.192%\n",
            "\n",
            "\n",
            "Tami decided to attend volleyball tryouts. Tami made the basketball team. And since then, they’ve been inseparable.\n",
            "196 / 1000   ---   0.196%\n",
            "\n",
            "\n",
            "Jeremy was working on his car. This time she was able to get the right part. And since it was his car, he was happy.\n",
            "197 / 1000   ---   0.197%\n",
            "\n",
            "\n",
            "Amy had heart palpitations after a lot of caffeine. By the time she arrived her heart felt much better. And since then, she’s been on the road to recovery.\n",
            "201 / 1000   ---   0.201%\n",
            "\n",
            "\n",
            "Amy was shopping for new slippers And since she didn't have any, she bought a pair. Amy ended up wearing the slippers until the sole fell out.\n",
            "206 / 1000   ---   0.206%\n",
            "\n",
            "\n",
            "Ned had always been shy. Ned is now more shy than ever before. And since that day, he's never been the same.\n",
            "210 / 1000   ---   0.21%\n",
            "\n",
            "\n",
            "All Kevin wanted in life was to buy his fiance a house. Kevin bought a house. And since then, he’s never looked back.\n",
            "215 / 1000   ---   0.215%\n",
            "\n",
            "\n",
            "Yesterday I got the Star Wars game from the iTunes Store. It was absolutely amazing. And since then I’ve been playing it constantly.\n",
            "219 / 1000   ---   0.219%\n",
            "\n",
            "\n",
            "Cindy had a guy friend. The guy never told his girlfriend he cheated. And since then, Cindy has never been the same.\n",
            "224 / 1000   ---   0.224%\n",
            "\n",
            "\n",
            "Ella wanted a kitten, and she had finally convinced her parents. And since they didn't have any, they took her to Petsmart. There, she found the perfect soft, loving little kitty!\n",
            "228 / 1000   ---   0.228%\n",
            "\n",
            "\n",
            "A scuba diver went to a lake to look at the fish. He found a wallet in the car and nothing else. And since then, no one has been able to find it.\n",
            "233 / 1000   ---   0.233%\n",
            "\n",
            "\n",
            "Bob had never tried coffee. He decided to get a black coffee with one sugar. And since he didn’t know any better, he ordered one.\n",
            "237 / 1000   ---   0.237%\n",
            "\n",
            "\n",
            "Sara was walking in front of her high school. She ended up tripping and falling on the sidewalk. And since then, she’s been in a lot of pain.\n",
            "239 / 1000   ---   0.239%\n",
            "\n",
            "\n",
            "Mary and Bill wanted to get a Christmas tree. Mary hung ornaments and lights, then sat back to admire her hard work. And since then, Mary and Bill have never looked back.\n",
            "243 / 1000   ---   0.243%\n",
            "\n",
            "\n",
            "Jeff painted his mailbox bright blue to make it look nice. Jeff limped home and painted his box green the next day. And since then, his mailbox has never been the same.\n",
            "248 / 1000   ---   0.248%\n",
            "\n",
            "\n",
            "Julie always wanted a kitten. She saw a kitten alone in a corner and knew this was the one for her. And since then, they have been inseparable.\n",
            "252 / 1000   ---   0.252%\n",
            "\n",
            "\n",
            "I was craving a BLT sub. My belly looks very round now. And since I haven't been eating much...\n",
            "256 / 1000   ---   0.256%\n",
            "\n",
            "\n",
            "Kelly and her family moved abroad. When she was three months in she was fluent. And since then, she’s never looked back.\n",
            "261 / 1000   ---   0.261%\n",
            "\n",
            "\n",
            "Johnny wanted to gain some confidence. Johnny was fulfilled and happy with his hard earned confidence. And since then, Johnny has never looked back.\n",
            "265 / 1000   ---   0.265%\n",
            "\n",
            "\n",
            "One morning, Jim was admiring his crush, Michaela. Later, Jim picked Michaela up for their date. And since then, they have been inseparable.\n",
            "270 / 1000   ---   0.27%\n",
            "\n",
            "\n",
            "Jester started out with a square piece of paper. Jester ended up with a paper swan, which he gave to his sister. And since then, Jester and his sister have been inseparable.\n",
            "275 / 1000   ---   0.275%\n",
            "\n",
            "\n",
            "Gina's phone died as they drove on the highway. Gina doesn't care about electronics. And since she doesn't care about electronics, neither do I.\n",
            "279 / 1000   ---   0.279%\n",
            "\n",
            "\n",
            "I had a group project to turn in. In addition, she led us to getting a good grade. And since then, she's been my best friend.\n",
            "282 / 1000   ---   0.282%\n",
            "\n",
            "\n",
            "Abby and Beck always read before bed. On the way up, they woke up, this is the last time she reads for them. And since she can't read to them anymore, she reads to me.\n",
            "287 / 1000   ---   0.287%\n",
            "\n",
            "\n",
            "Dave was friends with all the cool kids in school. Dave said no to Mike, and no one seemed to care. And since Mike didn’t care, Dave didn’t care either.\n",
            "291 / 1000   ---   0.291%\n",
            "\n",
            "\n",
            "I was drinking some soda out of a cup. I had expected soda, but drank the water by mistake. And since then, I’ve been drinking a lot of water.\n",
            "295 / 1000   ---   0.295%\n",
            "\n",
            "\n",
            "Nicole was spinning around in a circle with her classmates. All the kids started laughing. And since then, they haven't stopped.\n",
            "300 / 1000   ---   0.3%\n",
            "\n",
            "\n",
            "My favorite movie is the Wizard of Oz. One day, I decided to buy the movie. And since then, I’ve never looked back.\n",
            "304 / 1000   ---   0.304%\n",
            "\n",
            "\n",
            "The weather turned cold and foul by Jim's house. The ice melted a little so Jim added even more salt. And since it was so cold, he added more salt.\n",
            "307 / 1000   ---   0.307%\n",
            "\n",
            "\n",
            "Jamie becomes fat after Years of driving to the office. Jamie wishes he had begun biking Years ago. And since then, he’s never been the same.\n",
            "310 / 1000   ---   0.31%\n",
            "\n",
            "\n",
            "Gina woke up early in the morning. The hot air warmed the entire house. And since then, she hasn't been able to sleep.\n",
            "314 / 1000   ---   0.314%\n",
            "\n",
            "\n",
            "Sam thought he was the strongest one in the gym. He lifted more than Sam with one hand and took his money. And since then, Sam has never been the same.\n",
            "318 / 1000   ---   0.318%\n",
            "\n",
            "\n",
            "Sara wanted to make dinner for some guests. She had to order pizza for her friends instead. And since she didn’t have a recipe, she improvised.\n",
            "322 / 1000   ---   0.322%\n",
            "\n",
            "\n",
            "Ryan never wanted to go to summer camp but his parents made him. And since then, he's never looked back. Ryan couldn't wait to go next year to see his buddies.\n",
            "326 / 1000   ---   0.326%\n",
            "\n",
            "\n",
            "Tiffany loves browsing etsy stores. Tiffany soon after open up her very own etsy shop. And since then, she’s never looked back.\n",
            "329 / 1000   ---   0.329%\n",
            "\n",
            "\n",
            "Jane worked as a nanny. She continued working and did not care what they thought. And since then, she has never looked back.\n",
            "332 / 1000   ---   0.332%\n",
            "\n",
            "\n",
            "I really liked my friend. Indeed, I asked her out and she said no. And since then, I haven’t seen her again.\n",
            "337 / 1000   ---   0.337%\n",
            "\n",
            "\n",
            "Joan hadn't been feeling like herself lately. She and her partner were very excited. And since it was her birthday, they decided to celebrate.\n",
            "342 / 1000   ---   0.342%\n",
            "\n",
            "\n",
            "Mischa was out for dinner with friends. It was all free so it tasted that much better. And since Mischa was out for dinner, so was everyone else.\n",
            "347 / 1000   ---   0.347%\n",
            "\n",
            "\n",
            "Tim wanted to learn astronomy. Tim worked hard in school to become one. And since then, he’s never looked back.\n",
            "352 / 1000   ---   0.352%\n",
            "\n",
            "\n",
            "Lilly purchased the biggest turkey she could find. She had to fix burgers that she had in the freezer instead. And since she didn’t have a grill, she had to improvise.\n",
            "355 / 1000   ---   0.355%\n",
            "\n",
            "\n",
            "We went whale watching on a stormy day in August. Then the skies cleared as we made it to the channel and it was calm. And since then, we've gone whale watching every year.\n",
            "359 / 1000   ---   0.359%\n",
            "\n",
            "\n",
            "Tom was looking for Kenya on the globe. It turns out it was in Africa. And since then, he’s been looking for it ever since.\n",
            "363 / 1000   ---   0.363%\n",
            "\n",
            "\n",
            "Anna's roots were growing out. And since they were growing out, so was she. Now her roots were gone!\n",
            "367 / 1000   ---   0.367%\n",
            "\n",
            "\n",
            "Ina found a golden ring in the hall at school. Ina took the ring right to the Lost And Found office. And since then, she’s been looking for it ever since.\n",
            "372 / 1000   ---   0.372%\n",
            "\n",
            "\n",
            "Kelly opened up a gift from her grandmother. But to be nice, she told her grandmother she liked it. And since then, she hasn't looked back.\n",
            "377 / 1000   ---   0.377%\n",
            "\n",
            "\n",
            "I moved the razor over my face. Five more swipe made my face smooth. And since then, I haven’t looked back.\n",
            "380 / 1000   ---   0.38%\n",
            "\n",
            "\n",
            "Joshua was an absolute neat-freak. Joshua was so enraged, he threw a bar of soap at his friend. And since then, he’s never been the same.\n",
            "385 / 1000   ---   0.385%\n",
            "\n",
            "\n",
            "Rob's school was holding a Halloween costume contest. He was proud of himself, and decided to do even better next year. And since then, he's never looked back.\n",
            "389 / 1000   ---   0.389%\n",
            "\n",
            "\n",
            "Anne went on a guided jungle trek. Anna had a wonderful time on her jungle trek. And since then, she hasn't stopped talking about it.\n",
            "393 / 1000   ---   0.393%\n",
            "\n",
            "\n",
            "Charly signaled to her companion to be silent. They breathed a sigh of relief, and resumed their morning jog. And since then, they’ve never looked back.\n",
            "398 / 1000   ---   0.398%\n",
            "\n",
            "\n",
            "Jordan took every AP Course possible. Jordan finished with A's thanks to her anxiety pills. And since then, she's never looked back.\n",
            "402 / 1000   ---   0.402%\n",
            "\n",
            "\n",
            "Iris really needed a new hair treatment. When it was done it was great. And since then she's been doing great.\n",
            "407 / 1000   ---   0.407%\n",
            "\n",
            "\n",
            "Sal liked magic. The people watching him congratulated him. And since then, he’s never looked back.\n",
            "412 / 1000   ---   0.412%\n",
            "\n",
            "\n",
            "Will was looking for a new shirt. He was excited and bought the shirt. And since then he’s been wearing it every day.\n",
            "417 / 1000   ---   0.417%\n",
            "\n",
            "\n",
            "Peyton played football professionally. He was forced to quit playing. And since then, he hasn’t been the same.\n",
            "421 / 1000   ---   0.421%\n",
            "\n",
            "\n",
            "Anita didn't know her real father, but her step father was around. Then he took her outside and beat her with the water-hose. And since then, she's never been the same.\n",
            "425 / 1000   ---   0.425%\n",
            "\n",
            "\n",
            "Dominick use to hate school. He realized school was good for his brain. And since then, he's never looked back.\n",
            "429 / 1000   ---   0.429%\n",
            "\n",
            "\n",
            "I had a stroke in 2011. I found a one handed monocular and use it often. And since then, I’ve had a lot of fun.\n",
            "434 / 1000   ---   0.434%\n",
            "\n",
            "\n",
            "Jan plays tennis. She will continue to work on her backhand. And since it’s her birthday, she’ll celebrate.\n",
            "439 / 1000   ---   0.439%\n",
            "\n",
            "\n",
            "Matt had taken six different babysitting jobs in one day. By the end of the day, Matt was so tired he slept around 8 PM. And since then, he hadn’t been able to sleep at all.\n",
            "443 / 1000   ---   0.443%\n",
            "\n",
            "\n",
            "Terry was drafted at 18. The small amount was enough to help him get a taste for it. And since then, he hasn't looked back.\n",
            "445 / 1000   ---   0.445%\n",
            "\n",
            "\n",
            "Bob needed a new pair of socks. Bob returned everything the next day. And since then, he’s never looked back.\n",
            "450 / 1000   ---   0.45%\n",
            "\n",
            "\n",
            "Jane decided to pick a bunch of flowers. All Jane's neighbors complimented her beautiful flowers. And since then, Jane has never looked back.\n",
            "452 / 1000   ---   0.452%\n",
            "\n",
            "\n",
            "Wendy's brother loved to play tricks on her. And since he couldn't blow out her candles, he tried blowing them out. Wendy doubled up with laughter as he continued to try to blow them out\n",
            "456 / 1000   ---   0.456%\n",
            "\n",
            "\n",
            "Rachel was used to using the virtual keyboard to type on her iPad. Finally she found the perfect keyboard on Amazon. And since then, she’s never looked back.\n",
            "461 / 1000   ---   0.461%\n",
            "\n",
            "\n",
            "Ella had packed a picnic for her family. Ella was happy that the picnic wasn't ruined. And since the picnic wasn't ruined, Ella was happy.\n",
            "464 / 1000   ---   0.464%\n",
            "\n",
            "\n",
            "Lucy's grandma insisted Lucy clean her room. Her grandma was not impressed and cleaned the room again. And again. And again.\n",
            "468 / 1000   ---   0.468%\n",
            "\n",
            "\n",
            "I wanted some guacamole. And since I was hungry, I got it. Finally, the guacamole tasted delicious!\n",
            "473 / 1000   ---   0.473%\n",
            "\n",
            "\n",
            "Morgan was a candlemaker by trade. The bride and groom were beside themselves with happiness about it. And since then, they’ve never looked back.\n",
            "476 / 1000   ---   0.476%\n",
            "\n",
            "\n",
            "Judy was sad because she scored poorly on a her vision test. Judy was glad she got glasses. And since then, she has never looked back.\n",
            "480 / 1000   ---   0.48%\n",
            "\n",
            "\n",
            "The chef didn't know what to make for this week's special. Many customers loved the salmon salad but not the pasta dish. And since it was a special, the chef had to improvise.\n",
            "483 / 1000   ---   0.483%\n",
            "\n",
            "\n",
            "I worked at an elementary school in Hawaii. And since then, I’ve seen a lot of this. The kids were all excited and proud of the fruits of their labor!\n",
            "487 / 1000   ---   0.487%\n",
            "\n",
            "\n",
            "Sue was cooking spaghetti noodles. Sue burned the noodles. And since then, she's never been the same.\n",
            "490 / 1000   ---   0.49%\n",
            "\n",
            "\n",
            "Kim had been working extra hard for weeks. She was happy to get the promotion. And since then, she hasn’t looked back.\n",
            "494 / 1000   ---   0.494%\n",
            "\n",
            "\n",
            "Adam was only eight years old. However, he wasn't happy to see they cooked the lobsters. And since then, he hasn't been able to eat them.\n",
            "499 / 1000   ---   0.499%\n",
            "\n",
            "\n",
            "My dad always thought if there wasn't periods in USA it was wrong. But to this day he still believes that story. And since then, I've never had a period.\n",
            "504 / 1000   ---   0.504%\n",
            "\n",
            "\n",
            "Ann wanted to decorate her tree for Christmas. All of the ornaments broke. And since then, she hasn't been able to decorate.\n",
            "508 / 1000   ---   0.508%\n",
            "\n",
            "\n",
            "Lisa and Lexi were shopping for clothes at the mall. Lexi thought Lisa was too weird to hang around with anymore. And since Lisa didn’t want to hang around with Lexi, she left.\n",
            "512 / 1000   ---   0.512%\n",
            "\n",
            "\n",
            "Jan is excited when she gets a new job as a secretary. The next day Jan gets fired by her abusive boss. And since then, she’s never been the same.\n",
            "514 / 1000   ---   0.514%\n",
            "\n",
            "\n",
            "Barney went on his first hunting trip. Barney was able to make his first kill on the side of the road. And since then, he’s been hunting every chance he gets.\n",
            "517 / 1000   ---   0.517%\n",
            "\n",
            "\n",
            "Jane's co-worker agreed to help with an important project. Jane found him in the break room watching TV. And since then, they've been inseparable.\n",
            "521 / 1000   ---   0.521%\n",
            "\n",
            "\n",
            "My friend told me about a new bar in town. The man walked up to my friend and gave her his number. And since then, I’ve never heard from him again.\n",
            "526 / 1000   ---   0.526%\n",
            "\n",
            "\n",
            "Jasmine doesn't know how to play the guitar. Jasmine has become very good at playing the guitar. And since then, she's become very good at playing the guitar.\n",
            "530 / 1000   ---   0.53%\n",
            "\n",
            "\n",
            "Chad wanted to get a haircut. Chad was very angry but had to pay. And since it was his birthday, he got a haircut.\n",
            "535 / 1000   ---   0.535%\n",
            "\n",
            "\n",
            "The football team was losing by seven points in the fourth quarter. Sadly, the receiver did not catch the ball. And since he didn’t catch the ball, the game was over.\n",
            "539 / 1000   ---   0.539%\n",
            "\n",
            "\n",
            "Sarah got a car for her birthday. So she was happy about her decision. And since then, she hasn’t looked back.\n",
            "543 / 1000   ---   0.543%\n",
            "\n",
            "\n",
            "The boys made a campfire outside of the tent. The fire burned them. And since then, they’ve never been the same.\n",
            "548 / 1000   ---   0.548%\n",
            "\n",
            "\n",
            "Mike was talking to Molly. He was relieved to find out that it was not himself. And since then, he has not been able to sleep.\n",
            "551 / 1000   ---   0.551%\n",
            "\n",
            "\n",
            "My uncle is visiting us for the holidays. And since he's a guy, he likes to dress up. I can't wait to see what he wears tonight at dinner!\n",
            "556 / 1000   ---   0.556%\n",
            "\n",
            "\n",
            "Jon decided to travel to Indonesia. He had taught thousands of kids to speak basic English. And since then, he’s never looked back.\n",
            "559 / 1000   ---   0.559%\n",
            "\n",
            "\n",
            "For some reason Ben's toothbrush was by the kitchen sink. And since it was by the sink, Mr. Ben had accidentally brushed his teeth with it!\n",
            "563 / 1000   ---   0.563%\n",
            "\n",
            "\n",
            "Samson was a strong boy. He couldn't lift the car. And since then, he hasn't been able to.\n",
            "568 / 1000   ---   0.568%\n",
            "\n",
            "\n",
            "Jim decided he wanted to see a movie instead of going to work. His boss thankfully didn't recognize him because he was preoccupied. And since he didn't recognize him, he didn't get fired.\n",
            "572 / 1000   ---   0.572%\n",
            "\n",
            "\n",
            "Judd loved his monster truck. Judd realized that he should take better care of his truck. And since then, he's been taking better care of himself.\n",
            "577 / 1000   ---   0.577%\n",
            "\n",
            "\n",
            "Jane and Shawn were close friends. Shawn missed Jane every day. And since she died, he missed her too.\n",
            "581 / 1000   ---   0.581%\n",
            "\n",
            "\n",
            "Julie wanted to run her own business. And since she could, she did. Julie was very proud of herself on opening day.\n",
            "585 / 1000   ---   0.585%\n",
            "\n",
            "\n",
            "John's apartment was looking quite bleak. His new apartment looked great with the new plants. And since then, he hadn't looked back.\n",
            "590 / 1000   ---   0.59%\n",
            "\n",
            "\n",
            "Kallie loves to go swimming in the Ocean. She was very relieved and stress free afterwards. And since then, she loves to go swimming again.\n",
            "594 / 1000   ---   0.594%\n",
            "\n",
            "\n",
            "Sam wanted to get a pet. And since Fluffy was available, she was. Sam adopted Fluffy and took her home.\n",
            "599 / 1000   ---   0.599%\n",
            "\n",
            "\n",
            "Sasha was in her history class. Finally, her teacher revealed that it was all fake for a lesson. And since then, she hasn't been able to stop thinking about it.\n",
            "603 / 1000   ---   0.603%\n",
            "\n",
            "\n",
            "Brad and Allison love Texas country music. They both had a good time at the concert. And since then, they’ve been talking about it ever since.\n",
            "607 / 1000   ---   0.607%\n",
            "\n",
            "\n",
            "Gil was an inattentive driver. And since then, he's learned a lot. Gil now speaks publicly about the dangers of texting and driving.\n",
            "612 / 1000   ---   0.612%\n",
            "\n",
            "\n",
            "Mina was going through the attic and organizing things. She was impressed with the results of the meals she made. And since it was her birthday, she decided to celebrate.\n",
            "616 / 1000   ---   0.616%\n",
            "\n",
            "\n",
            "Bill recently applied for a new mortgage. Bill was excited to find out that he was approved for a mortgage. And since then, he's been working hard to pay it off.\n",
            "620 / 1000   ---   0.62%\n",
            "\n",
            "\n",
            "Lucy had a new toy. By the 5th day the toy sat abandoned in box full of abandoned toys. And since then the toy has not been played with.\n",
            "624 / 1000   ---   0.624%\n",
            "\n",
            "\n",
            "Gina decided to walk home from school. Gina knew she was in deep trouble. And since she didn’t know what else to do.\n",
            "628 / 1000   ---   0.628%\n",
            "\n",
            "\n",
            "Zach was getting tired of living with Jen. Weird how Zach is now happier than he was in a house with Jen. And since he moved out of Jen’s house, he’s happier than ever.\n",
            "631 / 1000   ---   0.631%\n",
            "\n",
            "\n",
            "Carla went to the movies with her boyfriend. Carla was very relieved. And since then, they have been inseparable.\n",
            "635 / 1000   ---   0.635%\n",
            "\n",
            "\n",
            "Elliott and Tim were on a high school tennis team. And since it was raining, Tim asked the coach why it was raining. The coach replied \"Was it raining on both sides of the court?\"\n",
            "639 / 1000   ---   0.639%\n",
            "\n",
            "\n",
            "Tim threw a ball for his puppy. From then on, Tim only threw the ball indoors. And since then, he's never thrown the ball outside again.\n",
            "642 / 1000   ---   0.642%\n",
            "\n",
            "\n",
            "Jenna gathered several ingredients in the kitchen. She continued to cook afterwards although her eye was still in pain. And since then, she has never looked back.\n",
            "647 / 1000   ---   0.647%\n",
            "\n",
            "\n",
            "Bill loved to play baseball. And since it was his birthday, he decided to play. Bill focused and hit a hard home run, winning his team the game!\n",
            "651 / 1000   ---   0.651%\n",
            "\n",
            "\n",
            "Being an only child is a bummer. And since I'm an only child, I care. I care because I'm tired of playing alone.\n",
            "654 / 1000   ---   0.654%\n",
            "\n",
            "\n",
            "Jerald wanted to buy his own car. Jerald got a mechanic to fix his car. And since then, Jerald has never looked back.\n",
            "658 / 1000   ---   0.658%\n",
            "\n",
            "\n",
            "Sophia had always been teased for being so pale. Now she was being teased for being orange and discolored!\n",
            "662 / 1000   ---   0.662%\n",
            "\n",
            "\n",
            "Jason wanted to invest in silver. He sold the silver dollars at a loss to get some money back. And since then, he’s been investing in gold.\n",
            "666 / 1000   ---   0.666%\n",
            "\n",
            "\n",
            "George took his niece to the bookstore. George and his niece had a fun outing. And since then, George and his niece have been inseparable.\n",
            "671 / 1000   ---   0.671%\n",
            "\n",
            "\n",
            "Beth hates to clean. She vacuums on Wednesday. And since then, she's been...\n",
            "676 / 1000   ---   0.676%\n",
            "\n",
            "\n",
            "Monday was making fudge with her mom. The fudge landed on a plate of cookies covering them all. And since then, they’ve been making fudge every day.\n",
            "680 / 1000   ---   0.68%\n",
            "\n",
            "\n",
            "Charles was 30-years-old and only remembered his father. But Charles was shocked at the resemblance, they were like twins. And since then, the two have been inseparable.\n",
            "684 / 1000   ---   0.684%\n",
            "\n",
            "\n",
            "I lived in Alaska. And since then, I've lived in Miami. I loved the year-round warm weather in Miami!\n",
            "688 / 1000   ---   0.688%\n",
            "\n",
            "\n",
            "Janice decided to change jobs. And since then, she hasn't looked back. Janice loves her new job as a teacher's aide.\n",
            "692 / 1000   ---   0.692%\n",
            "\n",
            "\n",
            "Thomas was plugging in his television. He then proceeded to pay more attention. And since then, he hasn’t looked back.\n",
            "696 / 1000   ---   0.696%\n",
            "\n",
            "\n",
            "Keith was really proud of his baseball card collection. The thought of selling made him too sad, however, so he said no. And since then, he’s been collecting baseball cards ever since.\n",
            "700 / 1000   ---   0.7%\n",
            "\n",
            "\n",
            "Steven was by a big flock of geese. He was able to get away. And since then, he’s been on the lam.\n",
            "705 / 1000   ---   0.705%\n",
            "\n",
            "\n",
            "Jake practiced skateboarding every single day. And since that day, he has never stopped. Jake thanked his best friend, and now practices harder than ever.\n",
            "710 / 1000   ---   0.71%\n",
            "\n",
            "\n",
            "Last Tuesday I bought an ice cream cake for a kid's birthday. Everyone was disappointed in me. And since then, I haven't been able to eat it.\n",
            "714 / 1000   ---   0.714%\n",
            "\n",
            "\n",
            "Bob enjoyed drinking wine. And since then, he’s been drinking beer. The wine tasted so much better!\n",
            "718 / 1000   ---   0.718%\n",
            "\n",
            "\n",
            "Sue was going to cook lasagna. And since I was home, I made it. It turned out perfectly!\n",
            "722 / 1000   ---   0.722%\n",
            "\n",
            "\n",
            "Steve and Cheryl were newly newlyweds. The cat has been a member of their family ever since. And since then, they’ve been inseparable.\n",
            "727 / 1000   ---   0.727%\n",
            "\n",
            "\n",
            "Kim had eaten deer meat without knowing what type of meat it was. She slowly drank a glass of water to calm her stomach. And since then, she hadn’t been able to stop thinking about it.\n",
            "732 / 1000   ---   0.732%\n",
            "\n",
            "\n",
            "It was the day off the big game. And since it was the day off, it was. The home team won!\n",
            "734 / 1000   ---   0.734%\n",
            "\n",
            "\n",
            "Arnold was scared of cats. Arnold loved his girlfriend's cats. And since then, they've been inseparable.\n",
            "738 / 1000   ---   0.738%\n",
            "\n",
            "\n",
            "Arnold was scared of cats. Arnold dumped his girlfriend. And since then, they've been inseparable.\n",
            "741 / 1000   ---   0.741%\n",
            "\n",
            "\n",
            "Shay was having an Easter egg hunt. And since it was Easter, the kids were excited. To their surprise, Shay gave prizes to them all!\n",
            "745 / 1000   ---   0.745%\n",
            "\n",
            "\n",
            "Fluffy the cat loved to chase squirrels. I had to take out a ladder to help Fluffy down. And since then, Fluffy has never been the same.\n",
            "750 / 1000   ---   0.75%\n",
            "\n",
            "\n",
            "Jill's favorite color was yellow. Jill loved it so much that she had a new favorite color. And since then, it's been her favorite color.\n",
            "754 / 1000   ---   0.754%\n",
            "\n",
            "\n",
            "Laurence walked into his home and smelled a really bad odor. Laurence threw those shoes in the dumpster and wore another pair. And since then, he’s been wearing the same pair of shoes.\n",
            "758 / 1000   ---   0.758%\n",
            "\n",
            "\n",
            "We tried to stay up late last night. We ran in hopes of improving our speed. And since we couldn't, we went to bed early.\n",
            "763 / 1000   ---   0.763%\n",
            "\n",
            "\n",
            "Lucy's grandma had gotten her favorite ice cream. When Lucy returned her ice cream was soup. And since then, Lucy has been eating soup every day.\n",
            "764 / 1000   ---   0.764%\n",
            "\n",
            "\n",
            "Jared loved online gaming. Until he realized he was on the wrong network. And since then, he’s never been the same.\n",
            "767 / 1000   ---   0.767%\n",
            "\n",
            "\n",
            "There was a young woman at my cafe who was ill. She soiled her pants, and everyone was sick from the stench. And since then, I haven’t seen her again.\n",
            "771 / 1000   ---   0.771%\n",
            "\n",
            "\n",
            "Jon made a three pointer and we were 20 points ahead of the other team. He made a layup, giving the other team the lead. And since then, we haven't lost a game.\n",
            "775 / 1000   ---   0.775%\n",
            "\n",
            "\n",
            "Linda thought the new guy was cute, but she wouldn't date him. He called her tease and Linda was scared and told her boss. And since then, she hasn't been able to stop thinking about him.\n",
            "779 / 1000   ---   0.779%\n",
            "\n",
            "\n",
            "Tom was having a pizza party for his birthday. But him and all his friends had fallen asleep. And since it was his birthday, he couldn’t sleep.\n",
            "784 / 1000   ---   0.784%\n",
            "\n",
            "\n",
            "Ben was a college student with no money. Ben took a job as a waiter. And since then, he’s never looked back.\n",
            "788 / 1000   ---   0.788%\n",
            "\n",
            "\n",
            "Matt was an avid golfer. Matt scoffed and laughed at the same time at his friend's behavior. And since then, Matt has never been the same.\n",
            "791 / 1000   ---   0.791%\n",
            "\n",
            "\n",
            "Heather was a caring and sweet little girl. She saved the kitten that day and the two were always together. And since then, they were inseparable from each other.\n",
            "794 / 1000   ---   0.794%\n",
            "\n",
            "\n",
            "I had a home repair business. Now, I have protection in case someone sues me. And since then, I’ve never been sued again.\n",
            "799 / 1000   ---   0.799%\n",
            "\n",
            "\n",
            "My girlfriend wanted to go roller skating for her birthday. I was never able to actually skate and had to give up eventually. And since then, I haven’t been able to go roller skating at all.\n",
            "803 / 1000   ---   0.803%\n",
            "\n",
            "\n",
            "The Miller family bought a new house. The Miller family celebrated with a cookout. And since then, things have only gotten better.\n",
            "808 / 1000   ---   0.808%\n",
            "\n",
            "\n",
            "Alex awoke to bright sunlight. She was late for school. And since she was late, so was everyone.\n",
            "811 / 1000   ---   0.811%\n",
            "\n",
            "\n",
            "Sara wanted the family to all have special Christmas stockings. The family all hung their personalized stockings on the mantle. And since it’s Christmas Eve, it’s the perfect time to decorate.\n",
            "814 / 1000   ---   0.814%\n",
            "\n",
            "\n",
            "Joe wanted to play football after school. And since he was so good at it, he was allowed to. Joe was so happy he was finally allowed to join the football team!\n",
            "818 / 1000   ---   0.818%\n",
            "\n",
            "\n",
            "Cara was having a craving for chocolate. Cara decided she'd have to write what she wanted on paper from now on. And since she didn't have a pen, she'd have to improvise.\n",
            "822 / 1000   ---   0.822%\n",
            "\n",
            "\n",
            "Yolanda was walking home from school. Yolanda was able to make it home before her bully could catch her. And since then, Yolanda has never been the same.\n",
            "826 / 1000   ---   0.826%\n",
            "\n",
            "\n",
            "Larry had an important job interview. Larry aced his job interview the next day. And since then, Larry has never looked back.\n",
            "831 / 1000   ---   0.831%\n",
            "\n",
            "\n",
            "There were six puppies running around my foyer. We did not like the dogs. And since then, we have not seen them again.\n",
            "834 / 1000   ---   0.834%\n",
            "\n",
            "\n",
            "Mike just finished eating a hamburger. 15 minutes later, he still felt that his hands weren't clean. And since then, he hasn't been able to wash them.\n",
            "837 / 1000   ---   0.837%\n",
            "\n",
            "\n",
            "Ned was Jewish and had a big nose. Ned felt much better about himself afterwards. And since then, his nose has never been the same.\n",
            "841 / 1000   ---   0.841%\n",
            "\n",
            "\n",
            "There was a crowd outside the clothing store that day. They devoured nearly everything and destroyed anything in their path. And since then, nothing has been the same.\n",
            "845 / 1000   ---   0.845%\n",
            "\n",
            "\n",
            "Arnie thought that he had failed his math test. His teacher explained that he had answered an extra credit question. And since he didn’t know the answer, he didn’t fail.\n",
            "849 / 1000   ---   0.849%\n",
            "\n",
            "\n",
            "I remember staying over at a friends house for a sleepover. His sister tried to scare me only to see me not wearing any boxers. And since then, I have never worn boxers in my life.\n",
            "854 / 1000   ---   0.854%\n",
            "\n",
            "\n",
            "Mary decided not to go to school and went to the mall instead. Mary's mom loved the sweater. And since it was her birthday, she bought it for her.\n",
            "858 / 1000   ---   0.858%\n",
            "\n",
            "\n",
            "Drew gave his girlfriend a pair of flowers. Drew called the ambulance and they gave her medication. And since then, the two have been inseparable.\n",
            "862 / 1000   ---   0.862%\n",
            "\n",
            "\n",
            "Todd has a huge test tomorrow. Todd decides to study as much as he can before tomorrow. And since it’s a test, he doesn’t want to fail.\n",
            "867 / 1000   ---   0.867%\n",
            "\n",
            "\n",
            "Ross and his friends needed to raise money for their school club. Ross and his friends raised even more money than they had planned. And since then, they have continued to raise money for their school club.\n",
            "869 / 1000   ---   0.869%\n",
            "\n",
            "\n",
            "Glenda's daughter was going through her terrible twos phase. The child sauntered away happily, leaving the mess to Glenda. And since then, Glenda has never looked back.\n",
            "874 / 1000   ---   0.874%\n",
            "\n",
            "\n",
            "Carmen was driving. Later that week she bought a set of tires. And since then, she’s never looked back.\n",
            "876 / 1000   ---   0.876%\n",
            "\n",
            "\n",
            "Lisa invited the girls to her house on the islands. She put up a fight and decided not to go. And since then, no one has heard from her.\n",
            "879 / 1000   ---   0.879%\n",
            "\n",
            "\n",
            "Tammy likes to crochet. Tammy was sad that she didn't have a present to give. And since she didn't have a present to give...\n",
            "883 / 1000   ---   0.883%\n",
            "\n",
            "\n",
            "I waited on the corner for the bus to come. I walked to the back and sat next to him. And since then, I haven’t seen him since.\n",
            "884 / 1000   ---   0.884%\n",
            "\n",
            "\n",
            "Tammy liked to listen to music. She had lost 50% of her hearing. And since she couldn’t hear, she couldn’t sing.\n",
            "888 / 1000   ---   0.888%\n",
            "\n",
            "\n",
            "It was a bright, warm day. And since then, I’ve never regretted going outside.\n",
            "892 / 1000   ---   0.892%\n",
            "\n",
            "\n",
            "Kelly wanted to learn to cook a German Chocolate Cake. In the end, Kelly cooked a deliciously beautiful cake. And since it was a German Chocolate Cake, it was delicious.\n",
            "897 / 1000   ---   0.897%\n",
            "\n",
            "\n",
            "We went on vacation and my nephew had issues with personal space. We finally had a good talk and he cut down on the hugging. And since then, he hasn't done it again.\n",
            "902 / 1000   ---   0.902%\n",
            "\n",
            "\n",
            "Ivy lost her cell phone while out grocery shopping. And since she didn’t have it with her, a stranger gave it to her. Ivy was so touched and pleased by human kindness!\n",
            "905 / 1000   ---   0.905%\n",
            "\n",
            "\n",
            "Ed had an extra class slot in his senior schedule. And since it was chemistry, he had to take it. Ed was so glad he had chosen chemistry!\n",
            "909 / 1000   ---   0.909%\n",
            "\n",
            "\n",
            "Anna wanted a new dress for her Christmas party. And since it was Anna's birthday, she got a new dress. All Anna's friends loved and complimented her new dress!\n",
            "913 / 1000   ---   0.913%\n",
            "\n",
            "\n",
            "I bought coffee on my way to work this morning. I had to turn around and change before work. And since I was on my way to work, I...\n",
            "918 / 1000   ---   0.918%\n",
            "\n",
            "\n",
            "Robert and his friends wanted to take a trip to Chipotle. Robert took some bites of his nachos and smiled in delight. And since then, he’s never looked back.\n",
            "922 / 1000   ---   0.922%\n",
            "\n",
            "\n",
            "Lily was having a contest at school. She really had wanted to win. And since she didn't, she didn't.\n",
            "927 / 1000   ---   0.927%\n",
            "\n",
            "\n",
            "Ellen used to live in the country. She was happy when the power came back on. And since then, she's been living in the city.\n",
            "932 / 1000   ---   0.932%\n",
            "\n",
            "\n",
            "Peter's dog was always excited to play with his bone. But it always found the bone to play with. And since then, Peter's dog has never looked back.\n",
            "933 / 1000   ---   0.933%\n",
            "\n",
            "\n",
            "Christina likes to go camping. Christina packed up on Sunday and drove home. And since then, she hasn’t looked back.\n",
            "938 / 1000   ---   0.938%\n",
            "\n",
            "\n",
            "Natalie's favorite movie is The Wizard of Oz. She got angry and stormed away. And since then, she hasn't looked back.\n",
            "943 / 1000   ---   0.943%\n",
            "\n",
            "\n",
            "In the car the whole family was excited. They decided to turn around. And since then, they haven’t looked back.\n",
            "948 / 1000   ---   0.948%\n",
            "\n",
            "\n",
            "In the car the whole family was excited. They couldn't wait to see grandma! And since then, they've been inseparable.\n",
            "951 / 1000   ---   0.951%\n",
            "\n",
            "\n",
            "Zach is trying to create some new habits. And since his birthday, he has been working out everyday. It has been 50 days, and Zach is still working out everyday!\n",
            "954 / 1000   ---   0.954%\n",
            "\n",
            "\n",
            "Ian badly had to go to the bathroom. Ian quickly walked away in embarrassment. And since then, he’s never been the same.\n",
            "959 / 1000   ---   0.959%\n",
            "\n",
            "\n",
            "Peter was excited to go to the Sanders rally in New Hampshire. He couldn't wait to vote for him. And since then, he's been a big Sanders supporter.\n",
            "963 / 1000   ---   0.963%\n",
            "\n",
            "\n",
            "Missy was going on a flight with her family. The whole flight was awesome. And since then, we've been inseparable.\n",
            "965 / 1000   ---   0.965%\n",
            "\n",
            "\n",
            "Ed likes to buy vintage records. He won't say what they cost him, though. And since he doesn't have to, he doesn't.\n",
            "969 / 1000   ---   0.969%\n",
            "\n",
            "\n",
            "Celeste and her friend wanted to go to the beach this Summer. She ended up buying it because it fit her well. And since it was on sale, she decided to keep it.\n",
            "974 / 1000   ---   0.974%\n",
            "\n",
            "\n",
            "Rowan was in the hospital because he broke his leg in an accident. Rowan recovered and was able to go home 5 weeks later. And since then, Rowan and his family have been doing great.\n",
            "979 / 1000   ---   0.979%\n",
            "\n",
            "\n",
            "I was outside with my wife one day. We let them go and went home. And since then, I've never seen them again.\n",
            "983 / 1000   ---   0.983%\n",
            "\n",
            "\n",
            "A man wanted to be an adept typist. His practice made perfect and he became a fast typist. And since then, he has been a fast typist.\n",
            "986 / 1000   ---   0.986%\n",
            "\n",
            "\n",
            "Jeanne had been divorced for years. Unfortunately, James was a confirmed old bachelor. And since he wasn’t married, he wasn’t eligible.\n",
            "991 / 1000   ---   0.991%\n",
            "\n",
            "\n",
            "Sam went to the doctor for a check up. He woke up in the recovery room feeling happy it was over. And since then, he’s never been the same.\n",
            "995 / 1000   ---   0.995%\n",
            "\n",
            "\n",
            "Liz made some homemade chili. She looked at the bottle and realized she accidentally added cinnamon. And since then, she’s been on a cinnamon kick.\n",
            "1000 / 1000   ---   1.0%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss8PHtUInGq6"
      },
      "source": [
        "with open('enthymeme.hypo', 'w') as f:\n",
        "    for item in target_lines:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rWhOcFrehG6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvk7VvAZd4x5",
        "outputId": "778e773b-4d62-4f2c-b5f1-d57d02f8a75f"
      },
      "source": [
        "target_lines"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"The Smiths were having photos done of the children. Ty's face lit up as he ran to the new toy, happily posing for photos. And since then, he's been playing with it ever since.\",\n",
              " 'Timmy was running fast to class. Timmy later found out that the exam was to be on time, he failed. And since then, Timmy has never been the same.',\n",
              " 'I was in line at the cinema. The boy proved her wrong by wetting himself. And since then, I’ve never been the same.',\n",
              " \"Zoey is an only child. And since she's an only child, she can't wait to meet her brother. When the baby is born, she goes to the hospital to meet her brother!\",\n",
              " \"Hannah sent her kids to go to the shoreline. As a result, they had to get out of the water and put on sunscreen. And since it was so hot, they didn't want to get wet.\",\n",
              " 'It was a cold night and the campers were tired and hungry. They went off to their tents after a few hours to sleep. And since then, they have never looked back.',\n",
              " 'Amy was in the park walking her dog Spot. Amy gave the mime a $5 to make up for being scared by Spot. And since then, the mime has been a regular at the park.',\n",
              " 'Jim was not confident in his home repair skills. Jim was so excited to learn a new skill. And since then, he has never looked back.',\n",
              " \"We used to go to the park all the time. He needed 19 Stitches that night. And since then, he's never been the same.\",\n",
              " \"Music was Kat's life. She completed her studies with honors, and is now a renowned pianist. And since then, she's been teaching others how to play.\",\n",
              " \"Paul and Jessica had been married for 15 Year's. Paul and Jessica finally went their separate ways. And since then, they haven't spoken to each other.\",\n",
              " 'Joe was an aspiring rap artist. Joe eventually became a local celebrity for his music. And since then, he’s never looked back.',\n",
              " \"Paul didn't want to go to school today. Now Paul is grounded for two weeks. And since he didn't want to go to school...\",\n",
              " 'Riley went to the store with her mother. Riley wore her cowboy boots to school the next day. And since then, she’s been wearing them every day.',\n",
              " 'Cindy was given a glass angel for Christmas. And since then, the angel has disappeared. Cindy has never been so devastated.',\n",
              " 'Brian had a crush on Teresa. He spent the rest of the day moping. And since then, he hasn’t been the same.',\n",
              " 'Ray hung a tire on a rope to make his daughter a swing. Ray laughed at the crazy situation. And since then, he’s been doing it every day.',\n",
              " 'Ray hung a tire on a rope to make his daughter a swing. Ray ran to his daughter to make sure she was okay. And since then, he has never looked back.',\n",
              " \"The hubby and I decided to have a date night tonight. We thanked Mama for her huge favor. And since then, we haven't looked back.\",\n",
              " \"Jordan and his girlfriend of three Year's decided to move in together. In the end, they decided to break up and move on. And since then, Jordan and his girlfriend have been inseparable.\",\n",
              " \"Becs sat down to start her novel on Sunday morning. For the next 6 hours, she wrote without stopping. And since then, she hasn't stopped.\",\n",
              " \"Mike spends a lot of his time on the internet. Now other people love the internet because of Mike's website. And since Mike loves the internet, so do we.\",\n",
              " 'I am visiting my mother-in-law this weekend. I ordered a big alarm clock that has good reviews. And since I am staying with my mother-in-law, I...',\n",
              " \"I was talking to my co-worker about a problem in our workplace. She ended up calling my boss and I was written up the next day. And since then, I haven't been able to get a job.\",\n",
              " 'Susan loves to ride her horse on the nearby trails. Susan needed stitches to close the cut on her head. And since then, she’s been able to ride her horse again.',\n",
              " \"Allan hated gym class. And since then, he's never been the same. Allan never hated gym class again!\",\n",
              " 'Tom loved to eat cheerios. Tom decided that cheerios are not good with water. And since then, cheerios are not good with water.',\n",
              " 'I ordered peonies for my girlfriend for Valentines day. They sent a new batch out the next day that were so great. And since then I’ve been getting them every year.',\n",
              " 'Cara had a crush on a boy in her English class named Ray. And since they were in the same class, they decided to do a project together. Soon they began dating - and they got a great grade on their project!',\n",
              " 'John is really good at playing chess. Eventually Mary won, Mark was right, and John was shocked. And since then, he’s never played chess again.',\n",
              " 'The children were inside playing when they heard music. The children excitedly bought ice cream cones. And since then, they have been enjoying their ice cream.',\n",
              " \"I was trying to tell my uncle a message. He didn't say a single word to me for the rest of the day. And since then, I haven't heard a word from him.\",\n",
              " 'I bought a great pair of red shoe at the shoe store. I ended up getting a white pair with no heels. And since then, I’ve been wearing them all the time.',\n",
              " 'After Mike was legally allowed to drink, he formed some bad habits. Mike decided to stop drinking alcohol from then on. And since then, he hasn’t had a drink since.',\n",
              " \"My aunt took a family portrait of everyone at the party. I didn't bother to ask for a copy and ended up walking away. And since then, I haven't been able to find it.\",\n",
              " 'Micah was at work. He was never able to find it. And since then, nothing has been the same.',\n",
              " 'Raya went to visit her family in Iran. She was glad to return home to America. And since then, she hasn’t looked back.',\n",
              " 'Amber and Rob had taken some time apart And since then, they’ve been spending more time together. Rob and Amber decided to move back in together',\n",
              " \"Gina's friend Tami had a folder that Gina wanted. And since Tami was gone, Gina didn't know where it was. So she decided she would try to find the folder on her own.\",\n",
              " 'It was my birthday. When I got home the party was set up for my brother. And since it was my birthday, I was invited.',\n",
              " 'Ritz was born in a large litter of 12 other puppies. Ritz was orphaned and had to be sent to a shelter. And since then, he’s become one of the city’s favorite dogs.',\n",
              " 'Joe lived a lonely life. Joe ordered some new cable channels, but was still unhappy. And since then, Joe’s life has never been the same.',\n",
              " \"Ginny was playing ball in the house. Ginny's mom grounded her. And since then, Ginny has been grounded.\",\n",
              " 'I wanted some cabinets to put in my bedroom. And since I didn’t have any, I decided to make them myself. After two months of work I had crafted some beautiful cabinets!',\n",
              " \"In the autumn, Susan planted some flower seeds in the backyard. She then dug up all her flowers. And since then, she hasn't been able to find any.\",\n",
              " 'Francis has always looked forward to learning how to drive. Francis decides he would rather take public transportation. And since then, he hasn’t looked back.',\n",
              " 'Amanda needed to get across town to the store. And since she didn’t have a car, she borrowed a friend’s. She got in and they were off to the store!',\n",
              " 'Mary makes candles. Mary felt unappreciated. And since then, nothing has changed.',\n",
              " 'Lara was having a birthday party. Tina was upset she was late. And since she was late, she was late.',\n",
              " 'Cory went down a very steep hill on a longboard. He was picked up by a car and taken to the hospital. And since then, he’s been in and out of the hospital.',\n",
              " 'Tami decided to attend volleyball tryouts. Tami made the basketball team. And since then, they’ve been inseparable.',\n",
              " 'Jeremy was working on his car. This time she was able to get the right part. And since it was his car, he was happy.',\n",
              " 'Amy had heart palpitations after a lot of caffeine. By the time she arrived her heart felt much better. And since then, she’s been on the road to recovery.',\n",
              " \"Amy was shopping for new slippers And since she didn't have any, she bought a pair. Amy ended up wearing the slippers until the sole fell out.\",\n",
              " \"Ned had always been shy. Ned is now more shy than ever before. And since that day, he's never been the same.\",\n",
              " 'All Kevin wanted in life was to buy his fiance a house. Kevin bought a house. And since then, he’s never looked back.',\n",
              " 'Yesterday I got the Star Wars game from the iTunes Store. It was absolutely amazing. And since then I’ve been playing it constantly.',\n",
              " 'Cindy had a guy friend. The guy never told his girlfriend he cheated. And since then, Cindy has never been the same.',\n",
              " \"Ella wanted a kitten, and she had finally convinced her parents. And since they didn't have any, they took her to Petsmart. There, she found the perfect soft, loving little kitty!\",\n",
              " 'A scuba diver went to a lake to look at the fish. He found a wallet in the car and nothing else. And since then, no one has been able to find it.',\n",
              " 'Bob had never tried coffee. He decided to get a black coffee with one sugar. And since he didn’t know any better, he ordered one.',\n",
              " 'Sara was walking in front of her high school. She ended up tripping and falling on the sidewalk. And since then, she’s been in a lot of pain.',\n",
              " 'Mary and Bill wanted to get a Christmas tree. Mary hung ornaments and lights, then sat back to admire her hard work. And since then, Mary and Bill have never looked back.',\n",
              " 'Jeff painted his mailbox bright blue to make it look nice. Jeff limped home and painted his box green the next day. And since then, his mailbox has never been the same.',\n",
              " 'Julie always wanted a kitten. She saw a kitten alone in a corner and knew this was the one for her. And since then, they have been inseparable.',\n",
              " \"I was craving a BLT sub. My belly looks very round now. And since I haven't been eating much...\",\n",
              " 'Kelly and her family moved abroad. When she was three months in she was fluent. And since then, she’s never looked back.',\n",
              " 'Johnny wanted to gain some confidence. Johnny was fulfilled and happy with his hard earned confidence. And since then, Johnny has never looked back.',\n",
              " 'One morning, Jim was admiring his crush, Michaela. Later, Jim picked Michaela up for their date. And since then, they have been inseparable.',\n",
              " 'Jester started out with a square piece of paper. Jester ended up with a paper swan, which he gave to his sister. And since then, Jester and his sister have been inseparable.',\n",
              " \"Gina's phone died as they drove on the highway. Gina doesn't care about electronics. And since she doesn't care about electronics, neither do I.\",\n",
              " \"I had a group project to turn in. In addition, she led us to getting a good grade. And since then, she's been my best friend.\",\n",
              " \"Abby and Beck always read before bed. On the way up, they woke up, this is the last time she reads for them. And since she can't read to them anymore, she reads to me.\",\n",
              " 'Dave was friends with all the cool kids in school. Dave said no to Mike, and no one seemed to care. And since Mike didn’t care, Dave didn’t care either.',\n",
              " 'I was drinking some soda out of a cup. I had expected soda, but drank the water by mistake. And since then, I’ve been drinking a lot of water.',\n",
              " \"Nicole was spinning around in a circle with her classmates. All the kids started laughing. And since then, they haven't stopped.\",\n",
              " 'My favorite movie is the Wizard of Oz. One day, I decided to buy the movie. And since then, I’ve never looked back.',\n",
              " \"The weather turned cold and foul by Jim's house. The ice melted a little so Jim added even more salt. And since it was so cold, he added more salt.\",\n",
              " 'Jamie becomes fat after Years of driving to the office. Jamie wishes he had begun biking Years ago. And since then, he’s never been the same.',\n",
              " \"Gina woke up early in the morning. The hot air warmed the entire house. And since then, she hasn't been able to sleep.\",\n",
              " 'Sam thought he was the strongest one in the gym. He lifted more than Sam with one hand and took his money. And since then, Sam has never been the same.',\n",
              " 'Sara wanted to make dinner for some guests. She had to order pizza for her friends instead. And since she didn’t have a recipe, she improvised.',\n",
              " \"Ryan never wanted to go to summer camp but his parents made him. And since then, he's never looked back. Ryan couldn't wait to go next year to see his buddies.\",\n",
              " 'Tiffany loves browsing etsy stores. Tiffany soon after open up her very own etsy shop. And since then, she’s never looked back.',\n",
              " 'Jane worked as a nanny. She continued working and did not care what they thought. And since then, she has never looked back.',\n",
              " 'I really liked my friend. Indeed, I asked her out and she said no. And since then, I haven’t seen her again.',\n",
              " \"Joan hadn't been feeling like herself lately. She and her partner were very excited. And since it was her birthday, they decided to celebrate.\",\n",
              " 'Mischa was out for dinner with friends. It was all free so it tasted that much better. And since Mischa was out for dinner, so was everyone else.',\n",
              " 'Tim wanted to learn astronomy. Tim worked hard in school to become one. And since then, he’s never looked back.',\n",
              " 'Lilly purchased the biggest turkey she could find. She had to fix burgers that she had in the freezer instead. And since she didn’t have a grill, she had to improvise.',\n",
              " \"We went whale watching on a stormy day in August. Then the skies cleared as we made it to the channel and it was calm. And since then, we've gone whale watching every year.\",\n",
              " 'Tom was looking for Kenya on the globe. It turns out it was in Africa. And since then, he’s been looking for it ever since.',\n",
              " \"Anna's roots were growing out. And since they were growing out, so was she. Now her roots were gone!\",\n",
              " 'Ina found a golden ring in the hall at school. Ina took the ring right to the Lost And Found office. And since then, she’s been looking for it ever since.',\n",
              " \"Kelly opened up a gift from her grandmother. But to be nice, she told her grandmother she liked it. And since then, she hasn't looked back.\",\n",
              " 'I moved the razor over my face. Five more swipe made my face smooth. And since then, I haven’t looked back.',\n",
              " 'Joshua was an absolute neat-freak. Joshua was so enraged, he threw a bar of soap at his friend. And since then, he’s never been the same.',\n",
              " \"Rob's school was holding a Halloween costume contest. He was proud of himself, and decided to do even better next year. And since then, he's never looked back.\",\n",
              " \"Anne went on a guided jungle trek. Anna had a wonderful time on her jungle trek. And since then, she hasn't stopped talking about it.\",\n",
              " 'Charly signaled to her companion to be silent. They breathed a sigh of relief, and resumed their morning jog. And since then, they’ve never looked back.',\n",
              " \"Jordan took every AP Course possible. Jordan finished with A's thanks to her anxiety pills. And since then, she's never looked back.\",\n",
              " \"Iris really needed a new hair treatment. When it was done it was great. And since then she's been doing great.\",\n",
              " 'Sal liked magic. The people watching him congratulated him. And since then, he’s never looked back.',\n",
              " 'Will was looking for a new shirt. He was excited and bought the shirt. And since then he’s been wearing it every day.',\n",
              " 'Peyton played football professionally. He was forced to quit playing. And since then, he hasn’t been the same.',\n",
              " \"Anita didn't know her real father, but her step father was around. Then he took her outside and beat her with the water-hose. And since then, she's never been the same.\",\n",
              " \"Dominick use to hate school. He realized school was good for his brain. And since then, he's never looked back.\",\n",
              " 'I had a stroke in 2011. I found a one handed monocular and use it often. And since then, I’ve had a lot of fun.',\n",
              " 'Jan plays tennis. She will continue to work on her backhand. And since it’s her birthday, she’ll celebrate.',\n",
              " 'Matt had taken six different babysitting jobs in one day. By the end of the day, Matt was so tired he slept around 8 PM. And since then, he hadn’t been able to sleep at all.',\n",
              " \"Terry was drafted at 18. The small amount was enough to help him get a taste for it. And since then, he hasn't looked back.\",\n",
              " 'Bob needed a new pair of socks. Bob returned everything the next day. And since then, he’s never looked back.',\n",
              " \"Jane decided to pick a bunch of flowers. All Jane's neighbors complimented her beautiful flowers. And since then, Jane has never looked back.\",\n",
              " \"Wendy's brother loved to play tricks on her. And since he couldn't blow out her candles, he tried blowing them out. Wendy doubled up with laughter as he continued to try to blow them out\",\n",
              " 'Rachel was used to using the virtual keyboard to type on her iPad. Finally she found the perfect keyboard on Amazon. And since then, she’s never looked back.',\n",
              " \"Ella had packed a picnic for her family. Ella was happy that the picnic wasn't ruined. And since the picnic wasn't ruined, Ella was happy.\",\n",
              " \"Lucy's grandma insisted Lucy clean her room. Her grandma was not impressed and cleaned the room again. And again. And again.\",\n",
              " 'I wanted some guacamole. And since I was hungry, I got it. Finally, the guacamole tasted delicious!',\n",
              " 'Morgan was a candlemaker by trade. The bride and groom were beside themselves with happiness about it. And since then, they’ve never looked back.',\n",
              " 'Judy was sad because she scored poorly on a her vision test. Judy was glad she got glasses. And since then, she has never looked back.',\n",
              " \"The chef didn't know what to make for this week's special. Many customers loved the salmon salad but not the pasta dish. And since it was a special, the chef had to improvise.\",\n",
              " 'I worked at an elementary school in Hawaii. And since then, I’ve seen a lot of this. The kids were all excited and proud of the fruits of their labor!',\n",
              " \"Sue was cooking spaghetti noodles. Sue burned the noodles. And since then, she's never been the same.\",\n",
              " 'Kim had been working extra hard for weeks. She was happy to get the promotion. And since then, she hasn’t looked back.',\n",
              " \"Adam was only eight years old. However, he wasn't happy to see they cooked the lobsters. And since then, he hasn't been able to eat them.\",\n",
              " \"My dad always thought if there wasn't periods in USA it was wrong. But to this day he still believes that story. And since then, I've never had a period.\",\n",
              " \"Ann wanted to decorate her tree for Christmas. All of the ornaments broke. And since then, she hasn't been able to decorate.\",\n",
              " 'Lisa and Lexi were shopping for clothes at the mall. Lexi thought Lisa was too weird to hang around with anymore. And since Lisa didn’t want to hang around with Lexi, she left.',\n",
              " 'Jan is excited when she gets a new job as a secretary. The next day Jan gets fired by her abusive boss. And since then, she’s never been the same.',\n",
              " 'Barney went on his first hunting trip. Barney was able to make his first kill on the side of the road. And since then, he’s been hunting every chance he gets.',\n",
              " \"Jane's co-worker agreed to help with an important project. Jane found him in the break room watching TV. And since then, they've been inseparable.\",\n",
              " 'My friend told me about a new bar in town. The man walked up to my friend and gave her his number. And since then, I’ve never heard from him again.',\n",
              " \"Jasmine doesn't know how to play the guitar. Jasmine has become very good at playing the guitar. And since then, she's become very good at playing the guitar.\",\n",
              " 'Chad wanted to get a haircut. Chad was very angry but had to pay. And since it was his birthday, he got a haircut.',\n",
              " 'The football team was losing by seven points in the fourth quarter. Sadly, the receiver did not catch the ball. And since he didn’t catch the ball, the game was over.',\n",
              " 'Sarah got a car for her birthday. So she was happy about her decision. And since then, she hasn’t looked back.',\n",
              " 'The boys made a campfire outside of the tent. The fire burned them. And since then, they’ve never been the same.',\n",
              " 'Mike was talking to Molly. He was relieved to find out that it was not himself. And since then, he has not been able to sleep.',\n",
              " \"My uncle is visiting us for the holidays. And since he's a guy, he likes to dress up. I can't wait to see what he wears tonight at dinner!\",\n",
              " 'Jon decided to travel to Indonesia. He had taught thousands of kids to speak basic English. And since then, he’s never looked back.',\n",
              " \"For some reason Ben's toothbrush was by the kitchen sink. And since it was by the sink, Mr. Ben had accidentally brushed his teeth with it!\",\n",
              " \"Samson was a strong boy. He couldn't lift the car. And since then, he hasn't been able to.\",\n",
              " \"Jim decided he wanted to see a movie instead of going to work. His boss thankfully didn't recognize him because he was preoccupied. And since he didn't recognize him, he didn't get fired.\",\n",
              " \"Judd loved his monster truck. Judd realized that he should take better care of his truck. And since then, he's been taking better care of himself.\",\n",
              " 'Jane and Shawn were close friends. Shawn missed Jane every day. And since she died, he missed her too.',\n",
              " 'Julie wanted to run her own business. And since she could, she did. Julie was very proud of herself on opening day.',\n",
              " \"John's apartment was looking quite bleak. His new apartment looked great with the new plants. And since then, he hadn't looked back.\",\n",
              " 'Kallie loves to go swimming in the Ocean. She was very relieved and stress free afterwards. And since then, she loves to go swimming again.',\n",
              " 'Sam wanted to get a pet. And since Fluffy was available, she was. Sam adopted Fluffy and took her home.',\n",
              " \"Sasha was in her history class. Finally, her teacher revealed that it was all fake for a lesson. And since then, she hasn't been able to stop thinking about it.\",\n",
              " 'Brad and Allison love Texas country music. They both had a good time at the concert. And since then, they’ve been talking about it ever since.',\n",
              " \"Gil was an inattentive driver. And since then, he's learned a lot. Gil now speaks publicly about the dangers of texting and driving.\",\n",
              " 'Mina was going through the attic and organizing things. She was impressed with the results of the meals she made. And since it was her birthday, she decided to celebrate.',\n",
              " \"Bill recently applied for a new mortgage. Bill was excited to find out that he was approved for a mortgage. And since then, he's been working hard to pay it off.\",\n",
              " 'Lucy had a new toy. By the 5th day the toy sat abandoned in box full of abandoned toys. And since then the toy has not been played with.',\n",
              " 'Gina decided to walk home from school. Gina knew she was in deep trouble. And since she didn’t know what else to do.',\n",
              " 'Zach was getting tired of living with Jen. Weird how Zach is now happier than he was in a house with Jen. And since he moved out of Jen’s house, he’s happier than ever.',\n",
              " 'Carla went to the movies with her boyfriend. Carla was very relieved. And since then, they have been inseparable.',\n",
              " 'Elliott and Tim were on a high school tennis team. And since it was raining, Tim asked the coach why it was raining. The coach replied \"Was it raining on both sides of the court?\"',\n",
              " \"Tim threw a ball for his puppy. From then on, Tim only threw the ball indoors. And since then, he's never thrown the ball outside again.\",\n",
              " 'Jenna gathered several ingredients in the kitchen. She continued to cook afterwards although her eye was still in pain. And since then, she has never looked back.',\n",
              " 'Bill loved to play baseball. And since it was his birthday, he decided to play. Bill focused and hit a hard home run, winning his team the game!',\n",
              " \"Being an only child is a bummer. And since I'm an only child, I care. I care because I'm tired of playing alone.\",\n",
              " 'Jerald wanted to buy his own car. Jerald got a mechanic to fix his car. And since then, Jerald has never looked back.',\n",
              " 'Sophia had always been teased for being so pale. Now she was being teased for being orange and discolored!',\n",
              " 'Jason wanted to invest in silver. He sold the silver dollars at a loss to get some money back. And since then, he’s been investing in gold.',\n",
              " 'George took his niece to the bookstore. George and his niece had a fun outing. And since then, George and his niece have been inseparable.',\n",
              " \"Beth hates to clean. She vacuums on Wednesday. And since then, she's been...\",\n",
              " 'Monday was making fudge with her mom. The fudge landed on a plate of cookies covering them all. And since then, they’ve been making fudge every day.',\n",
              " 'Charles was 30-years-old and only remembered his father. But Charles was shocked at the resemblance, they were like twins. And since then, the two have been inseparable.',\n",
              " \"I lived in Alaska. And since then, I've lived in Miami. I loved the year-round warm weather in Miami!\",\n",
              " \"Janice decided to change jobs. And since then, she hasn't looked back. Janice loves her new job as a teacher's aide.\",\n",
              " 'Thomas was plugging in his television. He then proceeded to pay more attention. And since then, he hasn’t looked back.',\n",
              " 'Keith was really proud of his baseball card collection. The thought of selling made him too sad, however, so he said no. And since then, he’s been collecting baseball cards ever since.',\n",
              " 'Steven was by a big flock of geese. He was able to get away. And since then, he’s been on the lam.',\n",
              " 'Jake practiced skateboarding every single day. And since that day, he has never stopped. Jake thanked his best friend, and now practices harder than ever.',\n",
              " \"Last Tuesday I bought an ice cream cake for a kid's birthday. Everyone was disappointed in me. And since then, I haven't been able to eat it.\",\n",
              " 'Bob enjoyed drinking wine. And since then, he’s been drinking beer. The wine tasted so much better!',\n",
              " 'Sue was going to cook lasagna. And since I was home, I made it. It turned out perfectly!',\n",
              " 'Steve and Cheryl were newly newlyweds. The cat has been a member of their family ever since. And since then, they’ve been inseparable.',\n",
              " 'Kim had eaten deer meat without knowing what type of meat it was. She slowly drank a glass of water to calm her stomach. And since then, she hadn’t been able to stop thinking about it.',\n",
              " 'It was the day off the big game. And since it was the day off, it was. The home team won!',\n",
              " \"Arnold was scared of cats. Arnold loved his girlfriend's cats. And since then, they've been inseparable.\",\n",
              " \"Arnold was scared of cats. Arnold dumped his girlfriend. And since then, they've been inseparable.\",\n",
              " 'Shay was having an Easter egg hunt. And since it was Easter, the kids were excited. To their surprise, Shay gave prizes to them all!',\n",
              " 'Fluffy the cat loved to chase squirrels. I had to take out a ladder to help Fluffy down. And since then, Fluffy has never been the same.',\n",
              " \"Jill's favorite color was yellow. Jill loved it so much that she had a new favorite color. And since then, it's been her favorite color.\",\n",
              " 'Laurence walked into his home and smelled a really bad odor. Laurence threw those shoes in the dumpster and wore another pair. And since then, he’s been wearing the same pair of shoes.',\n",
              " \"We tried to stay up late last night. We ran in hopes of improving our speed. And since we couldn't, we went to bed early.\",\n",
              " \"Lucy's grandma had gotten her favorite ice cream. When Lucy returned her ice cream was soup. And since then, Lucy has been eating soup every day.\",\n",
              " 'Jared loved online gaming. Until he realized he was on the wrong network. And since then, he’s never been the same.',\n",
              " 'There was a young woman at my cafe who was ill. She soiled her pants, and everyone was sick from the stench. And since then, I haven’t seen her again.',\n",
              " \"Jon made a three pointer and we were 20 points ahead of the other team. He made a layup, giving the other team the lead. And since then, we haven't lost a game.\",\n",
              " \"Linda thought the new guy was cute, but she wouldn't date him. He called her tease and Linda was scared and told her boss. And since then, she hasn't been able to stop thinking about him.\",\n",
              " 'Tom was having a pizza party for his birthday. But him and all his friends had fallen asleep. And since it was his birthday, he couldn’t sleep.',\n",
              " 'Ben was a college student with no money. Ben took a job as a waiter. And since then, he’s never looked back.',\n",
              " \"Matt was an avid golfer. Matt scoffed and laughed at the same time at his friend's behavior. And since then, Matt has never been the same.\",\n",
              " 'Heather was a caring and sweet little girl. She saved the kitten that day and the two were always together. And since then, they were inseparable from each other.',\n",
              " 'I had a home repair business. Now, I have protection in case someone sues me. And since then, I’ve never been sued again.',\n",
              " 'My girlfriend wanted to go roller skating for her birthday. I was never able to actually skate and had to give up eventually. And since then, I haven’t been able to go roller skating at all.',\n",
              " 'The Miller family bought a new house. The Miller family celebrated with a cookout. And since then, things have only gotten better.',\n",
              " 'Alex awoke to bright sunlight. She was late for school. And since she was late, so was everyone.',\n",
              " 'Sara wanted the family to all have special Christmas stockings. The family all hung their personalized stockings on the mantle. And since it’s Christmas Eve, it’s the perfect time to decorate.',\n",
              " 'Joe wanted to play football after school. And since he was so good at it, he was allowed to. Joe was so happy he was finally allowed to join the football team!',\n",
              " \"Cara was having a craving for chocolate. Cara decided she'd have to write what she wanted on paper from now on. And since she didn't have a pen, she'd have to improvise.\",\n",
              " 'Yolanda was walking home from school. Yolanda was able to make it home before her bully could catch her. And since then, Yolanda has never been the same.',\n",
              " 'Larry had an important job interview. Larry aced his job interview the next day. And since then, Larry has never looked back.',\n",
              " 'There were six puppies running around my foyer. We did not like the dogs. And since then, we have not seen them again.',\n",
              " \"Mike just finished eating a hamburger. 15 minutes later, he still felt that his hands weren't clean. And since then, he hasn't been able to wash them.\",\n",
              " 'Ned was Jewish and had a big nose. Ned felt much better about himself afterwards. And since then, his nose has never been the same.',\n",
              " 'There was a crowd outside the clothing store that day. They devoured nearly everything and destroyed anything in their path. And since then, nothing has been the same.',\n",
              " 'Arnie thought that he had failed his math test. His teacher explained that he had answered an extra credit question. And since he didn’t know the answer, he didn’t fail.',\n",
              " 'I remember staying over at a friends house for a sleepover. His sister tried to scare me only to see me not wearing any boxers. And since then, I have never worn boxers in my life.',\n",
              " \"Mary decided not to go to school and went to the mall instead. Mary's mom loved the sweater. And since it was her birthday, she bought it for her.\",\n",
              " 'Drew gave his girlfriend a pair of flowers. Drew called the ambulance and they gave her medication. And since then, the two have been inseparable.',\n",
              " 'Todd has a huge test tomorrow. Todd decides to study as much as he can before tomorrow. And since it’s a test, he doesn’t want to fail.',\n",
              " 'Ross and his friends needed to raise money for their school club. Ross and his friends raised even more money than they had planned. And since then, they have continued to raise money for their school club.',\n",
              " \"Glenda's daughter was going through her terrible twos phase. The child sauntered away happily, leaving the mess to Glenda. And since then, Glenda has never looked back.\",\n",
              " 'Carmen was driving. Later that week she bought a set of tires. And since then, she’s never looked back.',\n",
              " 'Lisa invited the girls to her house on the islands. She put up a fight and decided not to go. And since then, no one has heard from her.',\n",
              " \"Tammy likes to crochet. Tammy was sad that she didn't have a present to give. And since she didn't have a present to give...\",\n",
              " 'I waited on the corner for the bus to come. I walked to the back and sat next to him. And since then, I haven’t seen him since.',\n",
              " 'Tammy liked to listen to music. She had lost 50% of her hearing. And since she couldn’t hear, she couldn’t sing.',\n",
              " 'It was a bright, warm day. And since then, I’ve never regretted going outside.',\n",
              " 'Kelly wanted to learn to cook a German Chocolate Cake. In the end, Kelly cooked a deliciously beautiful cake. And since it was a German Chocolate Cake, it was delicious.',\n",
              " \"We went on vacation and my nephew had issues with personal space. We finally had a good talk and he cut down on the hugging. And since then, he hasn't done it again.\",\n",
              " 'Ivy lost her cell phone while out grocery shopping. And since she didn’t have it with her, a stranger gave it to her. Ivy was so touched and pleased by human kindness!',\n",
              " 'Ed had an extra class slot in his senior schedule. And since it was chemistry, he had to take it. Ed was so glad he had chosen chemistry!',\n",
              " \"Anna wanted a new dress for her Christmas party. And since it was Anna's birthday, she got a new dress. All Anna's friends loved and complimented her new dress!\",\n",
              " 'I bought coffee on my way to work this morning. I had to turn around and change before work. And since I was on my way to work, I...',\n",
              " 'Robert and his friends wanted to take a trip to Chipotle. Robert took some bites of his nachos and smiled in delight. And since then, he’s never looked back.',\n",
              " \"Lily was having a contest at school. She really had wanted to win. And since she didn't, she didn't.\",\n",
              " \"Ellen used to live in the country. She was happy when the power came back on. And since then, she's been living in the city.\",\n",
              " \"Peter's dog was always excited to play with his bone. But it always found the bone to play with. And since then, Peter's dog has never looked back.\",\n",
              " 'Christina likes to go camping. Christina packed up on Sunday and drove home. And since then, she hasn’t looked back.',\n",
              " \"Natalie's favorite movie is The Wizard of Oz. She got angry and stormed away. And since then, she hasn't looked back.\",\n",
              " 'In the car the whole family was excited. They decided to turn around. And since then, they haven’t looked back.',\n",
              " \"In the car the whole family was excited. They couldn't wait to see grandma! And since then, they've been inseparable.\",\n",
              " 'Zach is trying to create some new habits. And since his birthday, he has been working out everyday. It has been 50 days, and Zach is still working out everyday!',\n",
              " 'Ian badly had to go to the bathroom. Ian quickly walked away in embarrassment. And since then, he’s never been the same.',\n",
              " \"Peter was excited to go to the Sanders rally in New Hampshire. He couldn't wait to vote for him. And since then, he's been a big Sanders supporter.\",\n",
              " \"Missy was going on a flight with her family. The whole flight was awesome. And since then, we've been inseparable.\",\n",
              " \"Ed likes to buy vintage records. He won't say what they cost him, though. And since he doesn't have to, he doesn't.\",\n",
              " 'Celeste and her friend wanted to go to the beach this Summer. She ended up buying it because it fit her well. And since it was on sale, she decided to keep it.',\n",
              " 'Rowan was in the hospital because he broke his leg in an accident. Rowan recovered and was able to go home 5 weeks later. And since then, Rowan and his family have been doing great.',\n",
              " \"I was outside with my wife one day. We let them go and went home. And since then, I've never seen them again.\",\n",
              " 'A man wanted to be an adept typist. His practice made perfect and he became a fast typist. And since then, he has been a fast typist.',\n",
              " 'Jeanne had been divorced for years. Unfortunately, James was a confirmed old bachelor. And since he wasn’t married, he wasn’t eligible.',\n",
              " 'Sam went to the doctor for a check up. He woke up in the recovery room feeling happy it was over. And since then, he’s never been the same.',\n",
              " 'Liz made some homemade chili. She looked at the bottle and realized she accidentally added cinnamon. And since then, she’s been on a cinnamon kick.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}