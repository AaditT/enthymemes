{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BART_ZeroShot_Enthymemes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig5xzKFpLcHl",
        "outputId": "0dfcb41d-1bf2-4c46-a55b-3acf40f2489d"
      },
      "source": [
        "!pip install hydra-core"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 12.9MB/s \n",
            "\u001b[?25hCollecting omegaconf<2.1,>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core) (5.1.2)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 14.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1,>=2.0.5->hydra-core) (3.7.4.3)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core) (3.4.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=3cf2bfddcd59e6b907fdc8744b19b6214d2965b42a3025cb4c9eca37ff6d8ddc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, omegaconf, antlr4-python3-runtime, hydra-core\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 hydra-core-1.0.6 omegaconf-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgwV-3_hK1qo"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTeeynPMLMkY",
        "outputId": "e4866ba7-1416-4fc6-d6f9-375900107eb1"
      },
      "source": [
        "bart = torch.hub.load('pytorch/fairseq', 'bart.base')\n",
        "bart.eval()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/fairseq/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "cythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:369: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\n",
            "building 'fairseq.libbleu' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/fairseq\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/clib\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/clib/libbleu\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/fairseq\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.data_utils_fast' extension\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/data\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "creating build/lib.linux-x86_64-3.7/fairseq/data\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o -o build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.token_block_utils_fast' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o -o build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'fairseq.libnat' extension\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/clib/libnat\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so -> fairseq\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 422876945/422876945 [00:51<00:00, 8232763.45B/s] \n",
            "1042301B [00:00, 1227161.50B/s]\n",
            "456318B [00:00, 660514.46B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BARTHubInterface(\n",
              "  (models): ModuleList(\n",
              "    (0): BARTModel(\n",
              "      (encoder): TransformerEncoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(51201, 768, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
              "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (decoder): TransformerDecoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(51201, 768, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
              "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): TransformerDecoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (output_projection): Linear(in_features=768, out_features=51201, bias=False)\n",
              "      )\n",
              "      (classification_heads): ModuleDict()\n",
              "    )\n",
              "  )\n",
              "  (model): BARTModel(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (dropout_module): FairseqDropout()\n",
              "      (embed_tokens): Embedding(51201, 768, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (dropout_module): FairseqDropout()\n",
              "      (embed_tokens): Embedding(51201, 768, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerDecoderLayer(\n",
              "          (dropout_module): FairseqDropout()\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_dropout_module): FairseqDropout()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (output_projection): Linear(in_features=768, out_features=51201, bias=False)\n",
              "    )\n",
              "    (classification_heads): ModuleDict()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx_7HC8FLyOq"
      },
      "source": [
        "x = bart.fill_mask([\"Gina's friend Tami had a folder that Gina wanted. And since <mask> So she decided she would try to find the folder on her own.\"], topk=3, beam=10, match_source_len=False)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8DU6N2tPDya",
        "outputId": "0be3ec41-2eda-4369-f5b4-038d7a75ae3a"
      },
      "source": [
        "x[0]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"Gina's friend Tami had a folder that Gina wanted. And since Gina didn't have it, she couldn't find it. So she decided she would try to find the folder on her own.\",\n",
              "  tensor(-0.2027)),\n",
              " (\"Gina's friend Tami had a folder that Gina wanted. And since Gina didn't have a folder, she couldn't find it. So she decided she would try to find the folder on her own.\",\n",
              "  tensor(-0.2095)),\n",
              " (\"Gina's friend Tami had a folder that Gina wanted. And since Gina didn't have the folder, she couldn't find it. So she decided she would try to find the folder on her own.\",\n",
              "  tensor(-0.2116))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "_b_TWBVwQJ5f",
        "outputId": "eec48eb7-7283-41a2-e0ca-40ebec45f90e"
      },
      "source": [
        "x[0][1][0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Gina's friend Tami had a folder that Gina wanted. And since Gina didn't have a folder, she couldn't find it. So she decided she would try to find the folder on her own.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scry-iLoM4LD"
      },
      "source": [
        "val_lines_list = []\n",
        "val_file = open('val.source')\n",
        "val_lines = val_file.readlines()\n",
        "for line in val_lines:\n",
        "  val_lines_list.append(line)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPj8lzARNgro"
      },
      "source": [
        "val_lines_list2 = []\n",
        "for line in val_lines_list:\n",
        "  insert_str = \"And since <mask>.\"\n",
        "  line_blocks = line.split(\"#\")\n",
        "  if (len(line_blocks) == 2): # if there is only one #\n",
        "    new_str = line_blocks[0] + insert_str + line_blocks[1]\n",
        "    val_lines_list2.append(new_str)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKCfMYI2Qdot",
        "outputId": "a7b4c0f6-4881-4923-f076-4b349d4d5dde"
      },
      "source": [
        "len(val_lines_list2)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14313"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2RccekoQqgj",
        "outputId": "d6e162aa-2292-4b15-9f4d-cce3daf9857b"
      },
      "source": [
        "print(56/14313)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.003912527073290016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir5j6rI9PvWt",
        "outputId": "26c4c50e-ca11-4c8c-92cd-36f35e98923a"
      },
      "source": [
        "target_lines = []\n",
        "counter = 0\n",
        "limit = 1000\n",
        "for masked_line in val_lines_list2:\n",
        "  if (counter < limit):\n",
        "    x = bart.fill_mask([masked_line], topk=3, beam=10, match_source_len=False)\n",
        "    target_lines.append(x[0][0][0])\n",
        "    target_lines.append(x[0][1][0])\n",
        "    target_lines.append(x[0][2][0])\n",
        "    counter += 1\n",
        "    print(str(counter) + \" / \" + str(limit))\n",
        "  else:\n",
        "    break"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 1000\n",
            "2 / 1000\n",
            "3 / 1000\n",
            "4 / 1000\n",
            "5 / 1000\n",
            "6 / 1000\n",
            "7 / 1000\n",
            "8 / 1000\n",
            "9 / 1000\n",
            "10 / 1000\n",
            "11 / 1000\n",
            "12 / 1000\n",
            "13 / 1000\n",
            "14 / 1000\n",
            "15 / 1000\n",
            "16 / 1000\n",
            "17 / 1000\n",
            "18 / 1000\n",
            "19 / 1000\n",
            "20 / 1000\n",
            "21 / 1000\n",
            "22 / 1000\n",
            "23 / 1000\n",
            "24 / 1000\n",
            "25 / 1000\n",
            "26 / 1000\n",
            "27 / 1000\n",
            "28 / 1000\n",
            "29 / 1000\n",
            "30 / 1000\n",
            "31 / 1000\n",
            "32 / 1000\n",
            "33 / 1000\n",
            "34 / 1000\n",
            "35 / 1000\n",
            "36 / 1000\n",
            "37 / 1000\n",
            "38 / 1000\n",
            "39 / 1000\n",
            "40 / 1000\n",
            "41 / 1000\n",
            "42 / 1000\n",
            "43 / 1000\n",
            "44 / 1000\n",
            "45 / 1000\n",
            "46 / 1000\n",
            "47 / 1000\n",
            "48 / 1000\n",
            "49 / 1000\n",
            "50 / 1000\n",
            "51 / 1000\n",
            "52 / 1000\n",
            "53 / 1000\n",
            "54 / 1000\n",
            "55 / 1000\n",
            "56 / 1000\n",
            "57 / 1000\n",
            "58 / 1000\n",
            "59 / 1000\n",
            "60 / 1000\n",
            "61 / 1000\n",
            "62 / 1000\n",
            "63 / 1000\n",
            "64 / 1000\n",
            "65 / 1000\n",
            "66 / 1000\n",
            "67 / 1000\n",
            "68 / 1000\n",
            "69 / 1000\n",
            "70 / 1000\n",
            "71 / 1000\n",
            "72 / 1000\n",
            "73 / 1000\n",
            "74 / 1000\n",
            "75 / 1000\n",
            "76 / 1000\n",
            "77 / 1000\n",
            "78 / 1000\n",
            "79 / 1000\n",
            "80 / 1000\n",
            "81 / 1000\n",
            "82 / 1000\n",
            "83 / 1000\n",
            "84 / 1000\n",
            "85 / 1000\n",
            "86 / 1000\n",
            "87 / 1000\n",
            "88 / 1000\n",
            "89 / 1000\n",
            "90 / 1000\n",
            "91 / 1000\n",
            "92 / 1000\n",
            "93 / 1000\n",
            "94 / 1000\n",
            "95 / 1000\n",
            "96 / 1000\n",
            "97 / 1000\n",
            "98 / 1000\n",
            "99 / 1000\n",
            "100 / 1000\n",
            "101 / 1000\n",
            "102 / 1000\n",
            "103 / 1000\n",
            "104 / 1000\n",
            "105 / 1000\n",
            "106 / 1000\n",
            "107 / 1000\n",
            "108 / 1000\n",
            "109 / 1000\n",
            "110 / 1000\n",
            "111 / 1000\n",
            "112 / 1000\n",
            "113 / 1000\n",
            "114 / 1000\n",
            "115 / 1000\n",
            "116 / 1000\n",
            "117 / 1000\n",
            "118 / 1000\n",
            "119 / 1000\n",
            "120 / 1000\n",
            "121 / 1000\n",
            "122 / 1000\n",
            "123 / 1000\n",
            "124 / 1000\n",
            "125 / 1000\n",
            "126 / 1000\n",
            "127 / 1000\n",
            "128 / 1000\n",
            "129 / 1000\n",
            "130 / 1000\n",
            "131 / 1000\n",
            "132 / 1000\n",
            "133 / 1000\n",
            "134 / 1000\n",
            "135 / 1000\n",
            "136 / 1000\n",
            "137 / 1000\n",
            "138 / 1000\n",
            "139 / 1000\n",
            "140 / 1000\n",
            "141 / 1000\n",
            "142 / 1000\n",
            "143 / 1000\n",
            "144 / 1000\n",
            "145 / 1000\n",
            "146 / 1000\n",
            "147 / 1000\n",
            "148 / 1000\n",
            "149 / 1000\n",
            "150 / 1000\n",
            "151 / 1000\n",
            "152 / 1000\n",
            "153 / 1000\n",
            "154 / 1000\n",
            "155 / 1000\n",
            "156 / 1000\n",
            "157 / 1000\n",
            "158 / 1000\n",
            "159 / 1000\n",
            "160 / 1000\n",
            "161 / 1000\n",
            "162 / 1000\n",
            "163 / 1000\n",
            "164 / 1000\n",
            "165 / 1000\n",
            "166 / 1000\n",
            "167 / 1000\n",
            "168 / 1000\n",
            "169 / 1000\n",
            "170 / 1000\n",
            "171 / 1000\n",
            "172 / 1000\n",
            "173 / 1000\n",
            "174 / 1000\n",
            "175 / 1000\n",
            "176 / 1000\n",
            "177 / 1000\n",
            "178 / 1000\n",
            "179 / 1000\n",
            "180 / 1000\n",
            "181 / 1000\n",
            "182 / 1000\n",
            "183 / 1000\n",
            "184 / 1000\n",
            "185 / 1000\n",
            "186 / 1000\n",
            "187 / 1000\n",
            "188 / 1000\n",
            "189 / 1000\n",
            "190 / 1000\n",
            "191 / 1000\n",
            "192 / 1000\n",
            "193 / 1000\n",
            "194 / 1000\n",
            "195 / 1000\n",
            "196 / 1000\n",
            "197 / 1000\n",
            "198 / 1000\n",
            "199 / 1000\n",
            "200 / 1000\n",
            "201 / 1000\n",
            "202 / 1000\n",
            "204 / 1000\n",
            "205 / 1000\n",
            "206 / 1000\n",
            "207 / 1000\n",
            "208 / 1000\n",
            "209 / 1000\n",
            "210 / 1000\n",
            "211 / 1000\n",
            "212 / 1000\n",
            "213 / 1000\n",
            "214 / 1000\n",
            "215 / 1000\n",
            "216 / 1000\n",
            "217 / 1000\n",
            "218 / 1000\n",
            "219 / 1000\n",
            "220 / 1000\n",
            "221 / 1000\n",
            "222 / 1000\n",
            "223 / 1000\n",
            "224 / 1000\n",
            "225 / 1000\n",
            "226 / 1000\n",
            "227 / 1000\n",
            "228 / 1000\n",
            "229 / 1000\n",
            "230 / 1000\n",
            "231 / 1000\n",
            "232 / 1000\n",
            "233 / 1000\n",
            "234 / 1000\n",
            "235 / 1000\n",
            "236 / 1000\n",
            "237 / 1000\n",
            "238 / 1000\n",
            "239 / 1000\n",
            "240 / 1000\n",
            "241 / 1000\n",
            "242 / 1000\n",
            "243 / 1000\n",
            "244 / 1000\n",
            "245 / 1000\n",
            "246 / 1000\n",
            "247 / 1000\n",
            "248 / 1000\n",
            "249 / 1000\n",
            "250 / 1000\n",
            "251 / 1000\n",
            "252 / 1000\n",
            "253 / 1000\n",
            "254 / 1000\n",
            "255 / 1000\n",
            "256 / 1000\n",
            "257 / 1000\n",
            "258 / 1000\n",
            "259 / 1000\n",
            "260 / 1000\n",
            "261 / 1000\n",
            "262 / 1000\n",
            "263 / 1000\n",
            "264 / 1000\n",
            "265 / 1000\n",
            "266 / 1000\n",
            "267 / 1000\n",
            "268 / 1000\n",
            "269 / 1000\n",
            "270 / 1000\n",
            "271 / 1000\n",
            "272 / 1000\n",
            "273 / 1000\n",
            "274 / 1000\n",
            "275 / 1000\n",
            "276 / 1000\n",
            "277 / 1000\n",
            "278 / 1000\n",
            "279 / 1000\n",
            "280 / 1000\n",
            "281 / 1000\n",
            "282 / 1000\n",
            "283 / 1000\n",
            "284 / 1000\n",
            "285 / 1000\n",
            "286 / 1000\n",
            "287 / 1000\n",
            "288 / 1000\n",
            "289 / 1000\n",
            "290 / 1000\n",
            "291 / 1000\n",
            "292 / 1000\n",
            "293 / 1000\n",
            "294 / 1000\n",
            "295 / 1000\n",
            "296 / 1000\n",
            "297 / 1000\n",
            "298 / 1000\n",
            "299 / 1000\n",
            "300 / 1000\n",
            "301 / 1000\n",
            "302 / 1000\n",
            "303 / 1000\n",
            "304 / 1000\n",
            "305 / 1000\n",
            "306 / 1000\n",
            "307 / 1000\n",
            "308 / 1000\n",
            "309 / 1000\n",
            "310 / 1000\n",
            "311 / 1000\n",
            "312 / 1000\n",
            "313 / 1000\n",
            "314 / 1000\n",
            "315 / 1000\n",
            "316 / 1000\n",
            "317 / 1000\n",
            "318 / 1000\n",
            "319 / 1000\n",
            "320 / 1000\n",
            "321 / 1000\n",
            "322 / 1000\n",
            "323 / 1000\n",
            "324 / 1000\n",
            "325 / 1000\n",
            "326 / 1000\n",
            "327 / 1000\n",
            "328 / 1000\n",
            "329 / 1000\n",
            "330 / 1000\n",
            "331 / 1000\n",
            "332 / 1000\n",
            "333 / 1000\n",
            "334 / 1000\n",
            "335 / 1000\n",
            "336 / 1000\n",
            "337 / 1000\n",
            "338 / 1000\n",
            "339 / 1000\n",
            "340 / 1000\n",
            "341 / 1000\n",
            "342 / 1000\n",
            "343 / 1000\n",
            "344 / 1000\n",
            "345 / 1000\n",
            "346 / 1000\n",
            "347 / 1000\n",
            "348 / 1000\n",
            "349 / 1000\n",
            "350 / 1000\n",
            "351 / 1000\n",
            "352 / 1000\n",
            "353 / 1000\n",
            "354 / 1000\n",
            "355 / 1000\n",
            "356 / 1000\n",
            "357 / 1000\n",
            "358 / 1000\n",
            "359 / 1000\n",
            "360 / 1000\n",
            "361 / 1000\n",
            "362 / 1000\n",
            "363 / 1000\n",
            "364 / 1000\n",
            "365 / 1000\n",
            "366 / 1000\n",
            "367 / 1000\n",
            "368 / 1000\n",
            "369 / 1000\n",
            "370 / 1000\n",
            "371 / 1000\n",
            "372 / 1000\n",
            "373 / 1000\n",
            "374 / 1000\n",
            "375 / 1000\n",
            "376 / 1000\n",
            "377 / 1000\n",
            "378 / 1000\n",
            "379 / 1000\n",
            "380 / 1000\n",
            "381 / 1000\n",
            "382 / 1000\n",
            "383 / 1000\n",
            "384 / 1000\n",
            "385 / 1000\n",
            "386 / 1000\n",
            "387 / 1000\n",
            "388 / 1000\n",
            "389 / 1000\n",
            "390 / 1000\n",
            "391 / 1000\n",
            "392 / 1000\n",
            "393 / 1000\n",
            "394 / 1000\n",
            "395 / 1000\n",
            "396 / 1000\n",
            "397 / 1000\n",
            "398 / 1000\n",
            "399 / 1000\n",
            "400 / 1000\n",
            "401 / 1000\n",
            "402 / 1000\n",
            "403 / 1000\n",
            "404 / 1000\n",
            "405 / 1000\n",
            "406 / 1000\n",
            "407 / 1000\n",
            "408 / 1000\n",
            "409 / 1000\n",
            "410 / 1000\n",
            "411 / 1000\n",
            "412 / 1000\n",
            "413 / 1000\n",
            "414 / 1000\n",
            "415 / 1000\n",
            "416 / 1000\n",
            "417 / 1000\n",
            "418 / 1000\n",
            "419 / 1000\n",
            "420 / 1000\n",
            "421 / 1000\n",
            "422 / 1000\n",
            "423 / 1000\n",
            "424 / 1000\n",
            "425 / 1000\n",
            "426 / 1000\n",
            "427 / 1000\n",
            "428 / 1000\n",
            "429 / 1000\n",
            "430 / 1000\n",
            "431 / 1000\n",
            "432 / 1000\n",
            "433 / 1000\n",
            "434 / 1000\n",
            "435 / 1000\n",
            "436 / 1000\n",
            "437 / 1000\n",
            "438 / 1000\n",
            "439 / 1000\n",
            "440 / 1000\n",
            "441 / 1000\n",
            "442 / 1000\n",
            "443 / 1000\n",
            "444 / 1000\n",
            "445 / 1000\n",
            "446 / 1000\n",
            "447 / 1000\n",
            "448 / 1000\n",
            "449 / 1000\n",
            "450 / 1000\n",
            "451 / 1000\n",
            "452 / 1000\n",
            "453 / 1000\n",
            "454 / 1000\n",
            "455 / 1000\n",
            "456 / 1000\n",
            "457 / 1000\n",
            "458 / 1000\n",
            "459 / 1000\n",
            "460 / 1000\n",
            "461 / 1000\n",
            "462 / 1000\n",
            "463 / 1000\n",
            "464 / 1000\n",
            "465 / 1000\n",
            "466 / 1000\n",
            "467 / 1000\n",
            "468 / 1000\n",
            "469 / 1000\n",
            "470 / 1000\n",
            "471 / 1000\n",
            "472 / 1000\n",
            "473 / 1000\n",
            "474 / 1000\n",
            "475 / 1000\n",
            "476 / 1000\n",
            "477 / 1000\n",
            "478 / 1000\n",
            "479 / 1000\n",
            "480 / 1000\n",
            "481 / 1000\n",
            "482 / 1000\n",
            "483 / 1000\n",
            "484 / 1000\n",
            "485 / 1000\n",
            "486 / 1000\n",
            "487 / 1000\n",
            "488 / 1000\n",
            "489 / 1000\n",
            "490 / 1000\n",
            "491 / 1000\n",
            "492 / 1000\n",
            "493 / 1000\n",
            "494 / 1000\n",
            "495 / 1000\n",
            "496 / 1000\n",
            "497 / 1000\n",
            "498 / 1000\n",
            "499 / 1000\n",
            "500 / 1000\n",
            "501 / 1000\n",
            "502 / 1000\n",
            "503 / 1000\n",
            "504 / 1000\n",
            "505 / 1000\n",
            "506 / 1000\n",
            "507 / 1000\n",
            "508 / 1000\n",
            "509 / 1000\n",
            "510 / 1000\n",
            "511 / 1000\n",
            "512 / 1000\n",
            "513 / 1000\n",
            "514 / 1000\n",
            "515 / 1000\n",
            "516 / 1000\n",
            "517 / 1000\n",
            "518 / 1000\n",
            "519 / 1000\n",
            "520 / 1000\n",
            "521 / 1000\n",
            "522 / 1000\n",
            "523 / 1000\n",
            "524 / 1000\n",
            "525 / 1000\n",
            "526 / 1000\n",
            "527 / 1000\n",
            "528 / 1000\n",
            "529 / 1000\n",
            "530 / 1000\n",
            "531 / 1000\n",
            "532 / 1000\n",
            "533 / 1000\n",
            "534 / 1000\n",
            "535 / 1000\n",
            "536 / 1000\n",
            "537 / 1000\n",
            "538 / 1000\n",
            "539 / 1000\n",
            "540 / 1000\n",
            "541 / 1000\n",
            "542 / 1000\n",
            "543 / 1000\n",
            "544 / 1000\n",
            "545 / 1000\n",
            "546 / 1000\n",
            "547 / 1000\n",
            "548 / 1000\n",
            "549 / 1000\n",
            "550 / 1000\n",
            "551 / 1000\n",
            "552 / 1000\n",
            "553 / 1000\n",
            "554 / 1000\n",
            "555 / 1000\n",
            "556 / 1000\n",
            "557 / 1000\n",
            "558 / 1000\n",
            "559 / 1000\n",
            "560 / 1000\n",
            "561 / 1000\n",
            "562 / 1000\n",
            "563 / 1000\n",
            "564 / 1000\n",
            "565 / 1000\n",
            "566 / 1000\n",
            "567 / 1000\n",
            "568 / 1000\n",
            "569 / 1000\n",
            "570 / 1000\n",
            "571 / 1000\n",
            "572 / 1000\n",
            "573 / 1000\n",
            "574 / 1000\n",
            "575 / 1000\n",
            "576 / 1000\n",
            "577 / 1000\n",
            "578 / 1000\n",
            "579 / 1000\n",
            "580 / 1000\n",
            "581 / 1000\n",
            "582 / 1000\n",
            "583 / 1000\n",
            "584 / 1000\n",
            "585 / 1000\n",
            "586 / 1000\n",
            "587 / 1000\n",
            "588 / 1000\n",
            "589 / 1000\n",
            "590 / 1000\n",
            "591 / 1000\n",
            "592 / 1000\n",
            "593 / 1000\n",
            "594 / 1000\n",
            "595 / 1000\n",
            "596 / 1000\n",
            "597 / 1000\n",
            "598 / 1000\n",
            "599 / 1000\n",
            "600 / 1000\n",
            "601 / 1000\n",
            "602 / 1000\n",
            "603 / 1000\n",
            "604 / 1000\n",
            "605 / 1000\n",
            "606 / 1000\n",
            "607 / 1000\n",
            "608 / 1000\n",
            "609 / 1000\n",
            "610 / 1000\n",
            "611 / 1000\n",
            "612 / 1000\n",
            "613 / 1000\n",
            "614 / 1000\n",
            "615 / 1000\n",
            "616 / 1000\n",
            "617 / 1000\n",
            "618 / 1000\n",
            "619 / 1000\n",
            "620 / 1000\n",
            "621 / 1000\n",
            "622 / 1000\n",
            "623 / 1000\n",
            "624 / 1000\n",
            "625 / 1000\n",
            "626 / 1000\n",
            "627 / 1000\n",
            "628 / 1000\n",
            "629 / 1000\n",
            "630 / 1000\n",
            "631 / 1000\n",
            "632 / 1000\n",
            "633 / 1000\n",
            "634 / 1000\n",
            "635 / 1000\n",
            "636 / 1000\n",
            "637 / 1000\n",
            "638 / 1000\n",
            "639 / 1000\n",
            "640 / 1000\n",
            "641 / 1000\n",
            "642 / 1000\n",
            "643 / 1000\n",
            "644 / 1000\n",
            "645 / 1000\n",
            "646 / 1000\n",
            "647 / 1000\n",
            "648 / 1000\n",
            "649 / 1000\n",
            "650 / 1000\n",
            "651 / 1000\n",
            "652 / 1000\n",
            "653 / 1000\n",
            "654 / 1000\n",
            "655 / 1000\n",
            "656 / 1000\n",
            "657 / 1000\n",
            "658 / 1000\n",
            "659 / 1000\n",
            "660 / 1000\n",
            "661 / 1000\n",
            "662 / 1000\n",
            "663 / 1000\n",
            "664 / 1000\n",
            "665 / 1000\n",
            "666 / 1000\n",
            "667 / 1000\n",
            "668 / 1000\n",
            "669 / 1000\n",
            "670 / 1000\n",
            "671 / 1000\n",
            "672 / 1000\n",
            "673 / 1000\n",
            "674 / 1000\n",
            "675 / 1000\n",
            "676 / 1000\n",
            "677 / 1000\n",
            "678 / 1000\n",
            "679 / 1000\n",
            "680 / 1000\n",
            "681 / 1000\n",
            "682 / 1000\n",
            "683 / 1000\n",
            "684 / 1000\n",
            "685 / 1000\n",
            "686 / 1000\n",
            "687 / 1000\n",
            "688 / 1000\n",
            "689 / 1000\n",
            "690 / 1000\n",
            "691 / 1000\n",
            "692 / 1000\n",
            "693 / 1000\n",
            "694 / 1000\n",
            "695 / 1000\n",
            "696 / 1000\n",
            "697 / 1000\n",
            "698 / 1000\n",
            "699 / 1000\n",
            "700 / 1000\n",
            "701 / 1000\n",
            "702 / 1000\n",
            "703 / 1000\n",
            "704 / 1000\n",
            "705 / 1000\n",
            "706 / 1000\n",
            "707 / 1000\n",
            "708 / 1000\n",
            "709 / 1000\n",
            "710 / 1000\n",
            "711 / 1000\n",
            "712 / 1000\n",
            "713 / 1000\n",
            "714 / 1000\n",
            "715 / 1000\n",
            "716 / 1000\n",
            "717 / 1000\n",
            "718 / 1000\n",
            "719 / 1000\n",
            "720 / 1000\n",
            "721 / 1000\n",
            "722 / 1000\n",
            "723 / 1000\n",
            "724 / 1000\n",
            "725 / 1000\n",
            "726 / 1000\n",
            "727 / 1000\n",
            "728 / 1000\n",
            "729 / 1000\n",
            "730 / 1000\n",
            "731 / 1000\n",
            "732 / 1000\n",
            "733 / 1000\n",
            "734 / 1000\n",
            "735 / 1000\n",
            "736 / 1000\n",
            "737 / 1000\n",
            "738 / 1000\n",
            "739 / 1000\n",
            "740 / 1000\n",
            "741 / 1000\n",
            "742 / 1000\n",
            "743 / 1000\n",
            "744 / 1000\n",
            "745 / 1000\n",
            "746 / 1000\n",
            "747 / 1000\n",
            "748 / 1000\n",
            "749 / 1000\n",
            "750 / 1000\n",
            "751 / 1000\n",
            "752 / 1000\n",
            "753 / 1000\n",
            "754 / 1000\n",
            "755 / 1000\n",
            "756 / 1000\n",
            "757 / 1000\n",
            "758 / 1000\n",
            "759 / 1000\n",
            "760 / 1000\n",
            "761 / 1000\n",
            "762 / 1000\n",
            "763 / 1000\n",
            "764 / 1000\n",
            "765 / 1000\n",
            "766 / 1000\n",
            "767 / 1000\n",
            "768 / 1000\n",
            "769 / 1000\n",
            "770 / 1000\n",
            "771 / 1000\n",
            "772 / 1000\n",
            "773 / 1000\n",
            "774 / 1000\n",
            "775 / 1000\n",
            "776 / 1000\n",
            "777 / 1000\n",
            "778 / 1000\n",
            "779 / 1000\n",
            "780 / 1000\n",
            "781 / 1000\n",
            "782 / 1000\n",
            "783 / 1000\n",
            "784 / 1000\n",
            "785 / 1000\n",
            "786 / 1000\n",
            "787 / 1000\n",
            "788 / 1000\n",
            "789 / 1000\n",
            "790 / 1000\n",
            "791 / 1000\n",
            "792 / 1000\n",
            "793 / 1000\n",
            "794 / 1000\n",
            "795 / 1000\n",
            "796 / 1000\n",
            "797 / 1000\n",
            "798 / 1000\n",
            "799 / 1000\n",
            "800 / 1000\n",
            "801 / 1000\n",
            "802 / 1000\n",
            "803 / 1000\n",
            "804 / 1000\n",
            "805 / 1000\n",
            "806 / 1000\n",
            "807 / 1000\n",
            "808 / 1000\n",
            "809 / 1000\n",
            "810 / 1000\n",
            "811 / 1000\n",
            "812 / 1000\n",
            "813 / 1000\n",
            "814 / 1000\n",
            "815 / 1000\n",
            "816 / 1000\n",
            "817 / 1000\n",
            "818 / 1000\n",
            "819 / 1000\n",
            "820 / 1000\n",
            "821 / 1000\n",
            "822 / 1000\n",
            "823 / 1000\n",
            "824 / 1000\n",
            "825 / 1000\n",
            "826 / 1000\n",
            "827 / 1000\n",
            "828 / 1000\n",
            "829 / 1000\n",
            "830 / 1000\n",
            "831 / 1000\n",
            "832 / 1000\n",
            "833 / 1000\n",
            "834 / 1000\n",
            "835 / 1000\n",
            "836 / 1000\n",
            "837 / 1000\n",
            "838 / 1000\n",
            "839 / 1000\n",
            "840 / 1000\n",
            "841 / 1000\n",
            "842 / 1000\n",
            "843 / 1000\n",
            "844 / 1000\n",
            "845 / 1000\n",
            "846 / 1000\n",
            "847 / 1000\n",
            "848 / 1000\n",
            "849 / 1000\n",
            "850 / 1000\n",
            "851 / 1000\n",
            "852 / 1000\n",
            "853 / 1000\n",
            "854 / 1000\n",
            "855 / 1000\n",
            "856 / 1000\n",
            "857 / 1000\n",
            "858 / 1000\n",
            "859 / 1000\n",
            "860 / 1000\n",
            "861 / 1000\n",
            "862 / 1000\n",
            "863 / 1000\n",
            "864 / 1000\n",
            "865 / 1000\n",
            "866 / 1000\n",
            "867 / 1000\n",
            "868 / 1000\n",
            "869 / 1000\n",
            "870 / 1000\n",
            "871 / 1000\n",
            "872 / 1000\n",
            "873 / 1000\n",
            "874 / 1000\n",
            "875 / 1000\n",
            "876 / 1000\n",
            "877 / 1000\n",
            "878 / 1000\n",
            "879 / 1000\n",
            "880 / 1000\n",
            "881 / 1000\n",
            "882 / 1000\n",
            "883 / 1000\n",
            "884 / 1000\n",
            "885 / 1000\n",
            "886 / 1000\n",
            "887 / 1000\n",
            "888 / 1000\n",
            "889 / 1000\n",
            "890 / 1000\n",
            "891 / 1000\n",
            "892 / 1000\n",
            "893 / 1000\n",
            "894 / 1000\n",
            "895 / 1000\n",
            "896 / 1000\n",
            "897 / 1000\n",
            "898 / 1000\n",
            "899 / 1000\n",
            "900 / 1000\n",
            "901 / 1000\n",
            "902 / 1000\n",
            "903 / 1000\n",
            "904 / 1000\n",
            "905 / 1000\n",
            "906 / 1000\n",
            "907 / 1000\n",
            "908 / 1000\n",
            "909 / 1000\n",
            "910 / 1000\n",
            "911 / 1000\n",
            "912 / 1000\n",
            "913 / 1000\n",
            "914 / 1000\n",
            "915 / 1000\n",
            "916 / 1000\n",
            "917 / 1000\n",
            "918 / 1000\n",
            "919 / 1000\n",
            "920 / 1000\n",
            "921 / 1000\n",
            "922 / 1000\n",
            "923 / 1000\n",
            "924 / 1000\n",
            "925 / 1000\n",
            "926 / 1000\n",
            "927 / 1000\n",
            "928 / 1000\n",
            "929 / 1000\n",
            "930 / 1000\n",
            "931 / 1000\n",
            "932 / 1000\n",
            "933 / 1000\n",
            "934 / 1000\n",
            "935 / 1000\n",
            "936 / 1000\n",
            "937 / 1000\n",
            "938 / 1000\n",
            "939 / 1000\n",
            "940 / 1000\n",
            "941 / 1000\n",
            "942 / 1000\n",
            "943 / 1000\n",
            "944 / 1000\n",
            "945 / 1000\n",
            "946 / 1000\n",
            "947 / 1000\n",
            "948 / 1000\n",
            "949 / 1000\n",
            "950 / 1000\n",
            "951 / 1000\n",
            "952 / 1000\n",
            "953 / 1000\n",
            "954 / 1000\n",
            "955 / 1000\n",
            "956 / 1000\n",
            "957 / 1000\n",
            "958 / 1000\n",
            "959 / 1000\n",
            "960 / 1000\n",
            "961 / 1000\n",
            "962 / 1000\n",
            "963 / 1000\n",
            "964 / 1000\n",
            "965 / 1000\n",
            "966 / 1000\n",
            "967 / 1000\n",
            "968 / 1000\n",
            "969 / 1000\n",
            "970 / 1000\n",
            "971 / 1000\n",
            "972 / 1000\n",
            "973 / 1000\n",
            "974 / 1000\n",
            "975 / 1000\n",
            "976 / 1000\n",
            "977 / 1000\n",
            "978 / 1000\n",
            "979 / 1000\n",
            "980 / 1000\n",
            "981 / 1000\n",
            "982 / 1000\n",
            "983 / 1000\n",
            "984 / 1000\n",
            "985 / 1000\n",
            "986 / 1000\n",
            "987 / 1000\n",
            "988 / 1000\n",
            "989 / 1000\n",
            "990 / 1000\n",
            "991 / 1000\n",
            "992 / 1000\n",
            "993 / 1000\n",
            "994 / 1000\n",
            "995 / 1000\n",
            "996 / 1000\n",
            "997 / 1000\n",
            "998 / 1000\n",
            "999 / 1000\n",
            "1000 / 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss8PHtUInGq6"
      },
      "source": [
        "output_file = open(\"enthymeme.hypo\", \"w\")\n",
        "output_file.writelines(target_lines)"
      ],
      "execution_count": 56,
      "outputs": []
    }
  ]
}